[
  {
    "objectID": "demos/glove.html",
    "href": "demos/glove.html",
    "title": "Visualizing Twitter Messages with Emojis",
    "section": "",
    "text": "by: Eric S. Téllez\nThis example creates a visualization of Glove word embeddings using Embeddings.jl package to fetch them.\nNote: This example needs a lot of computing power; therefore you may want to set the environment variable JULIA_NUM_THREADS=auto before running julia.\nusing SimilaritySearch, SimSearchManifoldLearning, TextSearch, CodecZlib, JSON, DataFrames, Plots, StatsBase, LinearAlgebra, Markdown, Embeddings, Random\nusing Downloads: download\nemb, vocab = let emb = load_embeddings(GloVe{:en}, 2)  # you can change with any of the available embeddings in `Embeddings`\n    for c in eachcol(emb.embeddings)\n1        normalize!(c)\n    end\n\n2    Float16.(emb.embeddings), emb.vocab\nend\n\n3dist = NormalizedCosine_asf32()\n4vocab2id = Dict(w =&gt; i for (i, w) in enumerate(vocab))\n\n\n1\n\nNormalizes all vectors to have a unitary norm; this allow us to use the dot product as similarity (see point 3)\n\n2\n\nThe speed can be improved through memory’s bandwidth using less memory per vector; using Float16 as memory representation is a good idea even if your computer doesn’t support 16-bit floating point arithmetic natively.\n\n3\n\nSince we have unitary norm vectors we can simplify the cosine distance (i.e., \\(1 - dot(\\cdot, \\cdot)\\)); note that we are using Float16 and the suffix _asf32 will select a distance function that converts numbers to Float32 just before performing arithmetic operations.\n\n4\n\nInverse map from words to identifiers in vocab.\nNow we can create the index\n1index = SearchGraph(; dist, db=MatrixDatabase(emb))\nctx = SearchGraphContext(hyperparameters_callback=OptimizeParameters(MinRecall(0.99)))\n2index!(index, ctx)\n3optimize_index!(index, ctx, MinRecall(0.9))\n\n\n1\n\nDefines the index and the search context (caches and hyperparameters); particularly, we use a very high quality build MinRecall(0.99); high quality constructions yield to faster queries due to the underlying graph structure.\n\n2\n\nActual indexing procedure using the given search context.\n\n3\n\nOptimizing the index to trade quality and speed."
  },
  {
    "objectID": "demos/glove.html#umap-visualization",
    "href": "demos/glove.html#umap-visualization",
    "title": "Visualizing Twitter Messages with Emojis",
    "section": "UMAP Visualization",
    "text": "UMAP Visualization\n\nfunction normcolors(V)\n    min_, max_ = extrema(V)\n    V .= (V .- min_) ./ (max_ - min_)\n    V .= clamp.(V, 0, 1)\nend\n\nnormcolors(@view e3[1, :])\nnormcolors(@view e3[2, :])\nnormcolors(@view e3[3, :])\n\nlet C = [RGB(c[1], c[2], c[3]) for c in eachcol(e3)],\n    X = view(e2, 1, :),\n    Y = view(e2, 2, :)\n    scatter(X, Y, color=C, fmt=:png, alpha=0.2, size=(600, 600), ma=0.3, ms=2, msw=0, label=\"\", yticks=nothing, xticks=nothing, xaxis=false, yaxis=false)\nend\n\nplot!()\n\n\n\n\n\ne2, e3 = let min_dist=0.5f0,\n             k=12,\n             n_epochs=75,\n             neg_sample_rate=3,\n             tol=1e-3,\n             layout=RandomLayout()\n\n    @time \"Compute 2D UMAP model\" U2 = fit(UMAP, index; k, neg_sample_rate, layout, n_epochs, tol, min_dist)\n    @time \"Compute 3D UMAP model\" U3 = fit(U2, 3; neg_sample_rate, n_epochs, tol)\n    @time \"predicting 2D embeddings\" e2 = clamp.(predict(U2), -10f0, 10f0)\n    @time \"predicting 3D embeddings\" e3 = clamp.(predict(U3), -10f0, 10f0)\n    e2, e3\nend"
  },
  {
    "objectID": "demos/glove.html#final-notes",
    "href": "demos/glove.html#final-notes",
    "title": "Visualizing Twitter Messages with Emojis",
    "section": "Final notes",
    "text": "Final notes\nThis example shows how to index and search dense vector databases, in particular GloVe word embeddings using the cosine distance. Low dimensional projections are made with SimSearchManifoldLearning, note that SimilaritySearch is also used for computing the all \\(k\\) nearest neighbors needed by the UMAP model. Note that this notebook should be ran with several threads to reduce time costs."
  },
  {
    "objectID": "demos/glove.html#environment-and-dependencies",
    "href": "demos/glove.html#environment-and-dependencies",
    "title": "Visualizing Twitter Messages with Emojis",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "syn2d.html",
    "href": "syn2d.html",
    "title": "Visualizing MNIST database",
    "section": "",
    "text": "by: Eric S. Téllez\nThis demonstration shows in a 2D example the functionality of SearchGraph.\nusing SimilaritySearch, SimSearchManifoldLearning, Plots, StatsBase, LinearAlgebra, Markdown, Random\nn = 100_000\n\nM = randn(Float16, 2, n)\ndb = MatrixDatabase(M)\ndist = SqL2_asf32()\nsize(M)\nNow we can create the index\n1G = SearchGraph(; dist, db)\nctx = SearchGraphContext(hyperparameters_callback=OptimizeParameters(MinRecall(0.99)))\n2index!(G, ctx)\n3optimize_index!(G, ctx, MinRecall(0.9))\n\n\n1\n\nDefines the index and the search context (caches and hyperparameters); particularly, we use a very high quality build MinRecall(0.99); high quality constructions yield to faster queries due to the underlying graph structure.\n\n2\n\nActual indexing procedure using the given search context.\n\n3\n\nOptimizing the index to trade quality and speed."
  },
  {
    "objectID": "syn2d.html#the-set-of-queries",
    "href": "syn2d.html#the-set-of-queries",
    "title": "Visualizing MNIST database",
    "section": "The set of queries",
    "text": "The set of queries\nWe define a small set of queries being close to the border of the dataset and also in the most dense regions of the dataset.\n\nQ = [Float32[-2, -2], Float32[2, -2], Float32[-2, 0], Float32[-0, 2], Float32[0, 0],   Float32[-3, 3],  Float32[4, 4], Float32[1, 0.5]]\nI, D = searchbatch(G, ctx, VectorDatabase(Q), 30)\n\nPlease note how queries in low and high dense regions are located.\n\nscatter(view(M, 1, :), view(M, 2, :), fmt=:png, c=:cyan, ma=0.3, a=0.3, ms=1, msw=0)\n\nscatter!(getindex.(Q, 1), getindex.(Q, 2), c=:red, ma=0.7, a=0.7, ms=6, msw=0)\n\nfor c in eachcol(I)\n    X = M[:, c]\n    scatter!(view(X, 1, :), view(X, 2, :), c=:blue, ma=0.5, a=0.5, ms=2, msw=0)\n    #scatter!( c=:auto, ms=2)\nend\n\nplot!(legend=nothing)\n\nSince points are distributed in several regions with disparate density, their radii are also quite diverse. The next figure illustrates this effect."
  },
  {
    "objectID": "syn2d.html#distribution-of-distances-for-the-set-of-queries",
    "href": "syn2d.html#distribution-of-distances-for-the-set-of-queries",
    "title": "Visualizing MNIST database",
    "section": "Distribution of distances for the set of queries",
    "text": "Distribution of distances for the set of queries\n\nplot(D, m=:auto, yscale=:log10, title=\"knn distances for elements in Q\", fmt=:png)"
  },
  {
    "objectID": "syn2d.html#environment-and-dependencies",
    "href": "syn2d.html#environment-and-dependencies",
    "title": "Visualizing MNIST database",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "tutorials/solving-single-queries.html",
    "href": "tutorials/solving-single-queries.html",
    "title": "Solving single queries",
    "section": "",
    "text": "by: Eric S. Téllez\nusing SimilaritySearch\nThis example shows how to perform single queries instead of solving a batch of them. This is particularly useful for some applications, and we also show how they are solved, which could be used to avoid some memory allocations.\ndim = 8\ndb = MatrixDatabase(randn(Float32, dim, 10^4))\nqueries = db = MatrixDatabase(randn(Float32, dim, 100))\ndist = SqL2Distance()\nG = SearchGraph(; dist, db)\nctx = SearchGraphContext()\nindex!(G, ctx)\nSuppose you want to compute some \\(k\\) nearest neighbors, for this we use the structure KnnResult which is a priority queue of maximum size \\(k\\).\nfor _ in 1:10\n    res = KnnResult(3)\n\n    @time search(G, ctx, randn(Float32, dim), res)\n    @show minimum(res), maximum(res), argmin(res), argmax(res)\n    @show collect(IdView(res))\n    @show collect(DistView(res))\nend\n\n  0.183206 seconds (118.20 k allocations: 7.439 MiB, 99.96% compilation time)\n(minimum(res), maximum(res), argmin(res), argmax(res)) = (3.2125409f0, 5.9451365f0, 0x00000060, 0x0000005a)\ncollect(IdView(res)) = UInt32[0x00000060, 0x00000024, 0x0000005a]\ncollect(DistView(res)) = Float32[3.2125409, 4.102444, 5.9451365]\n  0.000019 seconds (3 allocations: 160 bytes)\n(minimum(res), maximum(res), argmin(res), argmax(res)) = (4.803366f0, 5.1825247f0, 0x00000061, 0x00000043)\ncollect(IdView(res)) = UInt32[0x00000061, 0x00000016, 0x00000043]\ncollect(DistView(res)) = Float32[4.803366, 5.1485715, 5.1825247]\n  0.000008 seconds (3 allocations: 160 bytes)\n(minimum(res), maximum(res), argmin(res), argmax(res)) = (4.0849f0, 5.552019f0, 0x00000026, 0x00000046)\ncollect(IdView(res)) = UInt32[0x00000026, 0x00000054, 0x00000046]\ncollect(DistView(res)) = Float32[4.0849, 4.1695414, 5.552019]\n  0.000004 seconds (3 allocations: 160 bytes)\n(minimum(res), maximum(res), argmin(res), argmax(res)) = (1.3791102f0, 2.5569458f0, 0x00000048, 0x00000024)\ncollect(IdView(res)) = UInt32[0x00000048, 0x0000003c, 0x00000024]\ncollect(DistView(res)) = Float32[1.3791102, 2.10572, 2.5569458]\n  0.000005 seconds (3 allocations: 160 bytes)\n(minimum(res), maximum(res), argmin(res), argmax(res)) = (3.8852205f0, 4.6184344f0, 0x00000048, 0x0000000f)\ncollect(IdView(res)) = UInt32[0x00000048, 0x00000028, 0x0000000f]\ncollect(DistView(res)) = Float32[3.8852205, 4.4886827, 4.6184344]\n  0.000003 seconds (3 allocations: 160 bytes)\n(minimum(res), maximum(res), argmin(res), argmax(res)) = (1.7899965f0, 1.9237554f0, 0x00000030, 0x00000039)\ncollect(IdView(res)) = UInt32[0x00000030, 0x0000003a, 0x00000039]\ncollect(DistView(res)) = Float32[1.7899965, 1.857374, 1.9237554]\n  0.000003 seconds (3 allocations: 160 bytes)\n(minimum(res), maximum(res), argmin(res), argmax(res)) = (2.4962053f0, 3.7744765f0, 0x00000048, 0x0000001c)\ncollect(IdView(res)) = UInt32[0x00000048, 0x00000038, 0x0000001c]\ncollect(DistView(res)) = Float32[2.4962053, 3.2587614, 3.7744765]\n  0.000004 seconds (3 allocations: 160 bytes)\n(minimum(res), maximum(res), argmin(res), argmax(res)) = (3.848355f0, 4.0870543f0, 0x00000022, 0x00000025)\ncollect(IdView(res)) = UInt32[0x00000022, 0x00000052, 0x00000025]\ncollect(DistView(res)) = Float32[3.848355, 3.9994867, 4.0870543]\n  0.000005 seconds (3 allocations: 160 bytes)\n(minimum(res), maximum(res), argmin(res), argmax(res)) = (6.1827874f0, 7.326759f0, 0x0000002a, 0x0000001c)\ncollect(IdView(res)) = UInt32[0x0000002a, 0x00000038, 0x0000001c]\ncollect(DistView(res)) = Float32[6.1827874, 6.242794, 7.326759]\n  0.000004 seconds (3 allocations: 160 bytes)\n(minimum(res), maximum(res), argmin(res), argmax(res)) = (7.2247243f0, 12.382386f0, 0x0000000c, 0x00000049)\ncollect(IdView(res)) = UInt32[0x0000000c, 0x00000024, 0x00000049]\ncollect(DistView(res)) = Float32[7.2247243, 10.467594, 12.382386]"
  },
  {
    "objectID": "tutorials/solving-single-queries.html#knnresult",
    "href": "tutorials/solving-single-queries.html#knnresult",
    "title": "Solving single queries",
    "section": "KnnResult",
    "text": "KnnResult\nThis structure is the container for the result and it is also used to specify the number of elements to retrieve. As mentioned before, it is a priority queue\n\n\nres = KnnResult(4)\npush_item!(res, 1, 10)\npush_item!(res, 2, 9)\npush_item!(res, 3, 8)\npush_item!(res, 4, 7)\npush_item!(res, 6, 5)\n@show res\n\n# it also supports removals\n@show :popfirst! =&gt; popfirst!(res)\npush_item!(res, 7, 0.1)\n@show :push_item! =&gt; res\n@show :pop! =&gt; pop!(res)\nres\n# It can be iterated\n\n@show collect(res)\n\nres = SimilaritySearch.KnnResult(SimilaritySearch.AdjacencyLists.IdWeight[SimilaritySearch.AdjacencyLists.IdWeight(0x00000006, 5.0f0), SimilaritySearch.AdjacencyLists.IdWeight(0x00000004, 7.0f0), SimilaritySearch.AdjacencyLists.IdWeight(0x00000003, 8.0f0), SimilaritySearch.AdjacencyLists.IdWeight(0x00000002, 9.0f0)], 4)\n:popfirst! =&gt; popfirst!(res) = :popfirst! =&gt; SimilaritySearch.AdjacencyLists.IdWeight(0x00000006, 5.0f0)\n:push_item! =&gt; res = :push_item! =&gt; SimilaritySearch.KnnResult(SimilaritySearch.AdjacencyLists.IdWeight[SimilaritySearch.AdjacencyLists.IdWeight(0x00000007, 0.1f0), SimilaritySearch.AdjacencyLists.IdWeight(0x00000004, 7.0f0), SimilaritySearch.AdjacencyLists.IdWeight(0x00000003, 8.0f0), SimilaritySearch.AdjacencyLists.IdWeight(0x00000002, 9.0f0)], 4)\n:pop! =&gt; pop!(res) = :pop! =&gt; SimilaritySearch.AdjacencyLists.IdWeight(0x00000002, 9.0f0)\ncollect(res) = SimilaritySearch.AdjacencyLists.IdWeight[SimilaritySearch.AdjacencyLists.IdWeight(0x00000007, 0.1f0), SimilaritySearch.AdjacencyLists.IdWeight(0x00000004, 7.0f0), SimilaritySearch.AdjacencyLists.IdWeight(0x00000003, 8.0f0)]\n\n\n3-element Vector{IdWeight}:\n IdWeight(0x00000007, 0.1f0)\n IdWeight(0x00000004, 7.0f0)\n IdWeight(0x00000003, 8.0f0)"
  },
  {
    "objectID": "tutorials/solving-single-queries.html#environment-and-dependencies",
    "href": "tutorials/solving-single-queries.html#environment-and-dependencies",
    "title": "Solving single queries",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "tutorials/incremental-construction.html",
    "href": "tutorials/incremental-construction.html",
    "title": "Incremental construction with SearchGraph",
    "section": "",
    "text": "by: Eric S. Téllez\nusing SimilaritySearch\nFor incremental construction we need a database backend that supports incremental insertions. Currently, there are two backends for this: DynamicMatrixDatabase and VectorDatabase:\ndim = 8\ndb = DynamicMatrixDatabase(Float32, dim) # or VectorDatabase(Vector{Float32})\ndist = L1Distance()\nit can use any distance function described in SimilaritySearch and Distances.jl, and in fact any SemiMetric as described in the later package. The index construction is made as follows:\nG = SearchGraph(; dist, db)\nctx = SearchGraphContext()\ninstead of index! we can use push_item! and append_items! functions\nfor _ in 1:10^4\n    push_item!(G, ctx, rand(Float32, dim))  # push_item! inserts one item at a time\nend\nwe can also use append_items! if we have a batch of items\nappend_items!(G, ctx, MatrixDatabase(rand(Float32, dim, 10^4))) # append_items! inserts many items at once\nNote that we used a MatrixDatabase to wrap the matrix to be inserted since it will be copied into the index. Now we have a populated index.\n@assert length(G) == 20_000\nthis will display a lot of information in the console, since as construction advances the hyperparameters of the index are adjusted.\nOnce the index is created, the index can solve nearest neighbor queries\n1Q = MatrixDatabase(rand(dim, 30))\n2k = 5\n3I, D = searchbatch(G, ctx, Q, k)\ndisplay((typeof(I), typeof(D)))\ndisplay((size(I), size(D)))\n\n\n1\n\nCreates the query\n\n2\n\nThe number of nearest neighbors to retrieve\n\n3\n\nSolve queries, returns neighbor identifiers and distances.\n\n\n\n\n(Matrix{Int32}, Matrix{Float32})\n\n\n((5, 30), (5, 30))"
  },
  {
    "objectID": "tutorials/incremental-construction.html#environment-and-dependencies",
    "href": "tutorials/incremental-construction.html#environment-and-dependencies",
    "title": "Incremental construction with SearchGraph",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "tutorials/automatic-hyperparameter-opt.html",
    "href": "tutorials/automatic-hyperparameter-opt.html",
    "title": "Automatic Hyperparameter Optimization",
    "section": "",
    "text": "by: Eric S. Téllez\nThis example optimizes different kinds of optimizations that allow different tradeoffs\n\nusing SimilaritySearch, Markdown\n\n1dim = 16\n2db = MatrixDatabase(rand(Float32, dim, 10^5))\n3queries = MatrixDatabase(rand(Float32, dim, 10^3))\n4dist = SqL2Distance()\n5k = 12\n\n\n1\n\nThe dimension to use in the synthetic data\n\n2\n\nThe synthetic database\n\n3\n\nThe synthetic queries\n\n4\n\nThe distance function; we will use the squared L2, which preserves the order of L2 but is faster to compute.\n\n5\n\nThe number of neighbors to retrieve"
  },
  {
    "objectID": "tutorials/automatic-hyperparameter-opt.html#automatic-hyperparameter-optimization",
    "href": "tutorials/automatic-hyperparameter-opt.html#automatic-hyperparameter-optimization",
    "title": "Automatic Hyperparameter Optimization",
    "section": "",
    "text": "by: Eric S. Téllez\nThis example optimizes different kinds of optimizations that allow different tradeoffs\n\nusing SimilaritySearch, Markdown\n\n1dim = 16\n2db = MatrixDatabase(rand(Float32, dim, 10^5))\n3queries = MatrixDatabase(rand(Float32, dim, 10^3))\n4dist = SqL2Distance()\n5k = 12\n\n\n1\n\nThe dimension to use in the synthetic data\n\n2\n\nThe synthetic database\n\n3\n\nThe synthetic queries\n\n4\n\nThe distance function; we will use the squared L2, which preserves the order of L2 but is faster to compute.\n\n5\n\nThe number of neighbors to retrieve"
  },
  {
    "objectID": "tutorials/automatic-hyperparameter-opt.html#computing-ground-truth",
    "href": "tutorials/automatic-hyperparameter-opt.html#computing-ground-truth",
    "title": "Automatic Hyperparameter Optimization",
    "section": "Computing ground truth",
    "text": "Computing ground truth\nWe will generate a ground truth with an exhaustive method.\n\ngoldI, goldD = searchbatch(ExhaustiveSearch(; db, dist), GenericContext(), queries, k)"
  },
  {
    "objectID": "tutorials/automatic-hyperparameter-opt.html#different-hyperparameter-optimization-strategies",
    "href": "tutorials/automatic-hyperparameter-opt.html#different-hyperparameter-optimization-strategies",
    "title": "Automatic Hyperparameter Optimization",
    "section": "Different hyperparameter optimization strategies",
    "text": "Different hyperparameter optimization strategies\nThe way of specifying the hyperparameter optimization strategy and objective is with a SearchGraphContext object, as follows:\n\nG1 = SearchGraph(; dist, db)\nC1 = SearchGraphContext(hyperparameters_callback=OptimizeParameters(MinRecall(0.99)))\nbuildtime1 = @elapsed index!(G1, C1)\n\nThe previous construction optimizes the construction to have a very high recall, which can be very costly but also produces a high quality index.\n\nG2 = SearchGraph(; dist, db)\nC2 = SearchGraphContext(hyperparameters_callback=OptimizeParameters(MinRecall(0.9)))\nbuildtime2 = @elapsed index!(G2, C2)\n\nsearch, searchbatch, index!, append_items!, and push_item! accept context arguments."
  },
  {
    "objectID": "tutorials/automatic-hyperparameter-opt.html#performances",
    "href": "tutorials/automatic-hyperparameter-opt.html#performances",
    "title": "Automatic Hyperparameter Optimization",
    "section": "Performances",
    "text": "Performances\nsearching times\n\ntime1 = @elapsed I1, D1 = searchbatch(G1, C1, queries, k)\ntime2 = @elapsed I2, D2 = searchbatch(G2, C2, queries, k)\nrecall1 = macrorecall(goldI, I1)\nrecall2 = macrorecall(goldI, I2)\n\nthe recall is an score value between 0 to 1 where values close to 1 indicate better qualities.\n\n\nbuild time:\n\nbuildtime1: 5.645083223\n\nbuildtime2: 0.525713899\n\n\nsearch time:\n\ntime1: 0.00217375\n\ntime2: 0.00136316\n\n\nrecall values:\n\nrecall1: 0.9748333333333307\n\nrecall2: 0.8294166666666665\n\n\n\n\n\nhere we can see smaller recalls than expected, and this is an effect of the difference between indexed elements (that are those objects used to perform the hyperparameter optimization). In any case, we 1can appreciate the differences among them, showing that high quality constructions may produce faster indexes; this is a consequence of the quality of the underlying structure. Contrary to this example, in higher dimensions or large datasets, we will obtain much higher construction times for high quality constructions."
  },
  {
    "objectID": "tutorials/automatic-hyperparameter-opt.html#optimizing-an-already-created-searchgraph-for-achieving-a-desired-quality",
    "href": "tutorials/automatic-hyperparameter-opt.html#optimizing-an-already-created-searchgraph-for-achieving-a-desired-quality",
    "title": "Automatic Hyperparameter Optimization",
    "section": "Optimizing an already created SearchGraph for achieving a desired quality",
    "text": "Optimizing an already created SearchGraph for achieving a desired quality\nThe hyperparameter optimization is performed in exponential stages while the SearchGraph is created; and therefore, the current hyperparameters could need an update. To optimize an already created SearchGraph we use optimize instead of index\nContext objects are special for construction since they encapsulate several hyperparameters; for searching it contains also caches but it can be shared among indexes; however, if the indexes have different sizes or you expect very different queries, it is better to maintain different context.\n\noptimize_index!(G1, C1, MinRecall(0.9))\noptimize_index!(G2, C1, MinRecall(0.9))\n\nafter optimizing the index its quality and speed are changed\n\ntime1 = @elapsed I1, D1 = searchbatch(G1, C1, queries, k)\ntime2 = @elapsed I2, D2 = searchbatch(G2, C1, queries, k)\n\nrecall1 = macrorecall(goldI, I1)\nrecall2 = macrorecall(goldI, I2)\n\nThese results on the following performances:\n\n\nbuild time:\n\nbuildtime1: 5.645083223\n\nbuildtime2: 0.525713899\n\n\nsearch time:\n\ntime1: 0.020584598\n\ntime2: 0.013707551\n\n\nrecall values:\n\nrecall1: 0.6276666666666668\n\nrecall2: 0.7295000000000004\n\n\n\n\n\nPlease note that faster searches are expected for indexes created for higher qualities; but the construction must be paid. Note that recall values are lower than expected, as we explained, due to differences in the distributions (more precisely between points already seen and not seen points)."
  },
  {
    "objectID": "tutorials/automatic-hyperparameter-opt.html#giving-more-realistic-queries-for-optimization",
    "href": "tutorials/automatic-hyperparameter-opt.html#giving-more-realistic-queries-for-optimization",
    "title": "Automatic Hyperparameter Optimization",
    "section": "Giving more realistic queries for optimization",
    "text": "Giving more realistic queries for optimization\nThe default optimization parameters use objects already indexed to tune the hyperparameters, which is too optimistic in real applications, since already indexed objects are particularly easy for this use. We can get a better optimization using external data:\n\noptqueries = MatrixDatabase(rand(Float32, dim, 64))\n\noptimize_index!(G1, C1, MinRecall(0.9); queries=optqueries)\noptimize_index!(G2, C1, MinRecall(0.9); queries=optqueries)\n\nafter optimizing the index its quality and speed are changed\n\ntime1 = @elapsed I1, D1 = searchbatch(G1, C1, queries, k)\ntime2 = @elapsed I2, D2 = searchbatch(G2, C1, queries, k)\n\nrecall1 = macrorecall(goldI, I1)\nrecall2 = macrorecall(goldI, I2)\n\nThese results on the following performances:\n\n\nbuild time:\n\nbuildtime1: 5.645083223\n\nbuildtime2: 0.525713899\n\n\nsearch time:\n\ntime1: 0.001382773\n\ntime2: 0.001392464\n\n\nrecall values:\n\nrecall1: 0.8797499999999979\n\nrecall2: 0.8976666666666652\n\n\n\n\n\nThese scores are much closer to those we are looking for.\nBe careful on doing optimize_index!(..., queries=queries) since this can yield to overfitting on your query set."
  },
  {
    "objectID": "tutorials/automatic-hyperparameter-opt.html#environment-and-dependencies",
    "href": "tutorials/automatic-hyperparameter-opt.html#environment-and-dependencies",
    "title": "Automatic Hyperparameter Optimization",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Our demonstrations are Pluto and Jupyter notebooks that can be used to replicate and interactively use SimilaritySearch. To make the demonstrations more attractive, we also make intensive use of visualizations based on non-linear dimensional reduction, These kind of algorithms use k nearest neighbors of a database as input to produce the low dimensional embedding. In particular, we use the SimSearchManifoldLearning which provides an implementation of UMAP and also defines the necessary functions to interoperate with the ManifoldLearning package."
  },
  {
    "objectID": "tutorials.html#introduction",
    "href": "tutorials.html#introduction",
    "title": "Tutorials",
    "section": "",
    "text": "Our demonstrations are Pluto and Jupyter notebooks that can be used to replicate and interactively use SimilaritySearch. To make the demonstrations more attractive, we also make intensive use of visualizations based on non-linear dimensional reduction, These kind of algorithms use k nearest neighbors of a database as input to produce the low dimensional embedding. In particular, we use the SimSearchManifoldLearning which provides an implementation of UMAP and also defines the necessary functions to interoperate with the ManifoldLearning package."
  },
  {
    "objectID": "tutorials.html#basic-usage",
    "href": "tutorials.html#basic-usage",
    "title": "Tutorials",
    "section": "Basic usage",
    "text": "Basic usage\n\nBasic usage - Euclidean distance, Random 2D points.\nIncremental construction - Manhattan distance, Random 8D points.\nAutomatic hyperparameter optimization - Squared Euclidean distance Random 16D points.\nSolving single queries - Euclidean distance, Random points.\nParallel construction and search - Euclidean distance, Random 2D points."
  },
  {
    "objectID": "tutorials.html#all-knn",
    "href": "tutorials.html#all-knn",
    "title": "Tutorials",
    "section": "All KNN",
    "text": "All KNN\n\nAll-KNN - Euclidean distance, Random points.\n\nWe provide two kinds of examples: - Pluto reactive notebooks that can run locally or online. - Jupyter notebooks are less reactive but they are great to visualize directly on github without running."
  },
  {
    "objectID": "tutorials.html#list-of-examples",
    "href": "tutorials.html#list-of-examples",
    "title": "Tutorials",
    "section": "List of examples",
    "text": "List of examples\nWe separate the examples by the kind of data, since some of the datasets are quite large and will require a lot of computer power. We also list how to connect SimilaritySearch with other packages that require solving k nearest neighbor queries.\n\nIndexing and visualizing synthetic and easily generated data\n\nSynthetic 8D: A tutorial-like Jupyter notebook that shows how to create an index on synthetic data and search it. Synthetic 8-dimensional dataset under L2.\nSynthetic 2D: A tutorial-like Jupyter notebook working on 2D synthetic dataset, also shows how the index works on different density regions of the database. Synthetic 2-dimensional dataset under L2.\n\n\n\nIndexing and visualizing real high dimensional datasets\nAll Pluto notebooks can work on mybinder and run without install anything in your computer, however, some examples uses large datasets and some of them require high computational resources (as many threads as you have); they could run slow in cloud computing services.\n\nIntegers as prime factors: A tutorial-like Jupyter notebook that produces an UMAP visualization of integers represented by its prime factors. It uses UMAP 2D and 3D projections. Very high dimension, based on the number of factors under the \\(n\\) integers; different user defined distances.\n\nPrimes\nPrimes (using ManifoldLearning)\n\nPrime gaps: Visualization of sequences of prime gaps to visualize them for searching patterns in this infinity source of objects. It uses 2D and 3D projections.\n\nSearch and UMAP projection Prime Gaps demo. It generates the dataset.\nThe end of Primes (using ManifoldLearning) contains a prime-gap visualization with Isomap.\n\nWIT: This example shows how to navigate, query, and visualize a small subset of the WIT dataset using Clip embeddings (vision & language). ~300K 512-dimensional vectors using the cosine distance.\n\nJupyter-based WIT demo, SimilaritySearch v0.10.\nPluto-based WIT demo, SimilaritySearch v0.8.\n\nGlove: Navigate and visualize semantic representations (Glove word embeddings), also can solve analogies. The vocabulary consists of 400K tokens represented as 100-dimensional vectors under the cosine distance.\n\nJupyter-based GloVe demo, SimilaritySearch v0.10.\nPluto-based GloVe demo, SimilaritySerach v0.8.\n\nMNIST: Navigation and visualization of the MNIST dataset of hand drawing numbers. It uses images directly as objects (28x28 matrices).\n\nJupyter-based MNIST demo, SimilaritySearch v0.10.\nPluto-based MNIST demo, SimilaritySearch v0.8.\nPluto-based MNIST animated projections, SimilaritySearch v0.8.\n\nWiktionary: Pluto notebook to navigate and query the Wiktionary vocabulary using Levenshtein distance (~1M words)\n\nJupyter-based Wiktionary demo, SimilaritySearch v0.10.\nPluto-based Wiktionary demo, SimilaritySearch v0.8.\n\nTweets: Pluto notebook to visualize a collection of Twitter’s Spanish messages with emojis using bag of words representations. 50K items.\n\nSearch and UMAP projection Emojispace demo.\n\n\nTODO: Cites and references ### Interoperating with other packages - Working with ManifoldLearning. This Pluto notebook implements the necessary structs and functions to solve knn queries for ManifoldLearning algorithms. We used two datasets, the first corresponding to the scurve and the second is for Prime gaps as time series."
  },
  {
    "objectID": "tutorials.html#search-demos-and-umap-visualization",
    "href": "tutorials.html#search-demos-and-umap-visualization",
    "title": "Tutorials",
    "section": "Search demos and UMAP visualization",
    "text": "Search demos and UMAP visualization\nThe demos are Pluto and [Jupyter](https://jupyter.org/] notebooks. Inside the repo’s root run the following commands.\n\n$ JULIA_NUM_THREADS=auto julia --project=.\n...\n\njulia&gt; using Pluto\n...\njulia&gt; Pluto.run(notebook=\"WIT/wit-demo.jl\")\n...\nor\n\n$ JULIA_NUM_THREADS=auto jupyter-lab .\nPlease recall that the first time you load a package Julia compiles it. Pluto notebooks also save its own environments and therefore it can use different package versions that those listed in the repo environment, which will cause installing and compiling packages the first time the notebooks run. Hopefully, this strategy improves the reproducibility at the cost of increasing loading times. Jupyter notebooks also contain the necessary package-manager instructions to improve reproducibility.\nNote: Pluto interface also allows loading notebooks, so you don’t need to exit and re-run to explore examples.\n\nVisualization\nMost visualizations are made with UMAP models using the SimSearchManifoldLearning package. These can be expensive and it is always recommended to run notebooks with all available threads."
  },
  {
    "objectID": "tutorials.html#initializing-the-environment",
    "href": "tutorials.html#initializing-the-environment",
    "title": "Tutorials",
    "section": "Initializing the environment",
    "text": "Initializing the environment\nSimilaritySearch.jl is writen in the Julia language you need to install it first in order to run them. After this it is necessary to install Pluto and/or IJulia (for Jupyter notebooks). If you need more information about how to install and use these notebooks, please see their respective sites.\n\nPluto\nIJulia"
  },
  {
    "objectID": "demos/mnist.html",
    "href": "demos/mnist.html",
    "title": "Visualizing MNIST database",
    "section": "",
    "text": "by: Eric S. Téllez\nThis example creates a visualization of the MNIST images (hand written digits) using MLDatasets.jl to retrieve it.\nNote: This example needs a lot of computing power; therefore you may want to set the environment variable JULIA_NUM_THREADS=auto before running julia.\nusing SimilaritySearch, SimSearchManifoldLearning, Plots, StatsBase, LinearAlgebra, Markdown, MLDatasets, Random\ndb, y, dist = let data = MNIST(split=:train)\n    T, y = data.features, data.targets\n    n = size(T, 3)\n    MatrixDatabase(Float32.(reshape(T, (28*28, n)))), y, SqL2_asf32()\nend\nNow we can create the index\n1index = SearchGraph(; dist, db)\nctx = SearchGraphContext(hyperparameters_callback=OptimizeParameters(MinRecall(0.99)))\n2index!(index, ctx)\n3optimize_index!(index, ctx, MinRecall(0.95))\n\n\n1\n\nDefines the index and the search context (caches and hyperparameters); particularly, we use a very high quality build MinRecall(0.99); high quality constructions yield to faster queries due to the underlying graph structure.\n\n2\n\nActual indexing procedure using the given search context.\n\n3\n\nOptimizing the index to trade quality and speed."
  },
  {
    "objectID": "demos/mnist.html#umap-visualization",
    "href": "demos/mnist.html#umap-visualization",
    "title": "Visualizing MNIST database",
    "section": "UMAP Visualization",
    "text": "UMAP Visualization\n\nfunction normcolors(V)\n    min_, max_ = extrema(V)\n    V .= (V .- min_) ./ (max_ - min_)\n    V .= clamp.(V, 0, 1)\nend\n\nnormcolors(@view e3[1, :])\nnormcolors(@view e3[2, :])\nnormcolors(@view e3[3, :])\n\nlet C = [RGB(c[1], c[2], c[3]) for c in eachcol(e3)],\n    X = view(e2, 1, :),\n    Y = view(e2, 2, :)\n    scatter(X, Y, color=C, fmt=:png, alpha=0.2, size=(600, 600), ma=0.3, ms=2, msw=0, label=\"\", yticks=nothing, xticks=nothing, xaxis=false, yaxis=false)\n    for i in 1:100\n        j = rand(1:length(y))\n        annotate!(X[j], Y[j], text(y[j], :black, :right, 8, \"noto\"))\n    end\nend\n\nplot!()\n\n\n\n\n\ne2, e3 = let min_dist=0.5f0,\n             k=7,\n             n_epochs=75,\n             neg_sample_rate=3,\n             tol=1e-3,\n             layout=SpectralLayout()\n\n    @time \"Compute 2D UMAP model\" U2 = fit(UMAP, index; k, neg_sample_rate, layout, n_epochs, tol, min_dist)\n    @time \"Compute 3D UMAP model\" U3 = fit(U2, 3; neg_sample_rate, n_epochs, tol)\n    @time \"predicting 2D embeddings\" e2 = clamp.(predict(U2), -10f0, 10f0)\n    @time \"predicting 3D embeddings\" e3 = clamp.(predict(U3), -10f0, 10f0)\n    e2, e3\nend"
  },
  {
    "objectID": "demos/mnist.html#final-notes",
    "href": "demos/mnist.html#final-notes",
    "title": "Visualizing MNIST database",
    "section": "Final notes",
    "text": "Final notes\nThis example shows how to index and visualize the MNIST dataset using UMAP low dimensional projections. Low dimensional projections are made with SimSearchManifoldLearning, note that SimilaritySearch is also used for computing the all \\(k\\) nearest neighbors needed by the UMAP model. Note that this notebook should be ran with several threads to reduce time costs."
  },
  {
    "objectID": "demos/mnist.html#environment-and-dependencies",
    "href": "demos/mnist.html#environment-and-dependencies",
    "title": "Visualizing MNIST database",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "tutorials/allknn.html",
    "href": "tutorials/allknn.html",
    "title": "Computing all \\(k\\) nearest neighbors",
    "section": "",
    "text": "by: Eric S. Téllez"
  },
  {
    "objectID": "tutorials/allknn.html#introduction",
    "href": "tutorials/allknn.html#introduction",
    "title": "Computing all \\(k\\) nearest neighbors",
    "section": "Introduction",
    "text": "Introduction\nComputing the \\(k\\) nearest neighbors of a dataset (a.k.a. allknn) is a useful task to take knowledge of a given dataset. This is a fundamental problem for some clustering algorithms and non-linear dimensional reduction algorithms.\nGiven a metric database \\((X, dist)\\) and a relatively small \\(k\\) value, the goal is to compute \\(\\{ knn(x) \\mid x \\in X \\}\\) taking into account that each \\(x_i \\in X\\), and therefore, \\(x_i\\) should be removed from the \\(i\\)-th \\(knn\\) result set.\nSolving allknn fast and accuratelly is the goal of this example."
  },
  {
    "objectID": "tutorials/allknn.html#initializing-our-notebook",
    "href": "tutorials/allknn.html#initializing-our-notebook",
    "title": "Computing all \\(k\\) nearest neighbors",
    "section": "Initializing our notebook",
    "text": "Initializing our notebook\nThe first step is to load our basic packages\n\nusing SimilaritySearch, Markdown\n\nwe will use a synthetic dataset\n\nfunction synthetic_benchmark(n, dim)\n1    db = MatrixDatabase(randn(Float32, dim, n))\n2    dist = SqL2Distance()\n3    (; db, dist)\nend\n\n\n1\n\nGenerate \\(n\\) random vectors, of \\(dim\\) dimension. Note that we wrap the matrix as MatrixDatabase to let our index that this is a database; this is necessary since we typically can change the type of objects and distance functions to work.\n\n2\n\nThe squared Euclidean distance; it preserves the order than plain Euclidean distance, but it is faster.\n\n3\n\nReturns a named tuple with the dataset and the distance.\n\n\n\n\n\n1B = synthetic_benchmark(10^5, 16)\n2k = 8\n3etime = @elapsed eknns, edists = allknn(ExhaustiveSearch(; B.db, B.dist),  GenericContext(), k)\n\n\n1\n\nCreates a synthetic dataset of dimension \\(16\\) and \\(10^5\\) points.\n\n2\n\nDefines we will be fetching this number of neighbors.\n\n3\n\nCreates a gold standard for test and compare.\n\n\n\n\n\nG = SearchGraph(; B.dist, B.db)\n2ctx = SearchGraphContext()\n3itime = @elapsed index!(G, ctx)\n4atime = @elapsed knns, dists = allknn(G, ctx, k)\n\n\n2\n\nDefines the SearchGraph index; it does not indexes anything yet!\n\n3\n\nDefines a search context, it contains several hyperparameters that will be applied for the indexing process, default values just work for now.\n\n4\n\nThe actual indexing."
  },
  {
    "objectID": "tutorials/allknn.html#differences-between-allknng-k-and-searchbatchg-x-k",
    "href": "tutorials/allknn.html#differences-between-allknng-k-and-searchbatchg-x-k",
    "title": "Computing all \\(k\\) nearest neighbors",
    "section": "Differences between allknn(G, k) and searchbatch(G, X, k)",
    "text": "Differences between allknn(G, k) and searchbatch(G, X, k)\nWe can solve similarly with searchbatch but self-references should be removed later, and more important, allknn use special pivoting/boosting strategies that yields to faster searches.\n\nstime = @elapsed sknns, sdists = searchbatch(G, ctx, B.db, k)"
  },
  {
    "objectID": "tutorials/allknn.html#comparing-solutions",
    "href": "tutorials/allknn.html#comparing-solutions",
    "title": "Computing all \\(k\\) nearest neighbors",
    "section": "Comparing solutions",
    "text": "Comparing solutions\nWe can measure the quality of SearchGraph in its different modalities against the exhaustive search (exact) solution.\n\nallknn_recall = macrorecall(eknns, knns)\nsearch_recall = macrorecall(eknns, sknns)\n\n\n\nTimes:\n\nindexing: 5.627129575\n\nallknn with SearchGraph: 0.337121587\n\nsearchbatch with SearchGraph: 0.417784617\n\nallknn with Exhaustivesearch: 7.140021421\n\n\nThe search and recall tradeoff:\n\nallknn (SearchGraph): 0.93524625\n\nsearchbatch (SearchGraph): 0.9381225"
  },
  {
    "objectID": "tutorials/allknn.html#final-notes",
    "href": "tutorials/allknn.html#final-notes",
    "title": "Computing all \\(k\\) nearest neighbors",
    "section": "Final notes",
    "text": "Final notes\nExhaustive search will fetch the exact solution but it has a higher cost and this could be more notorious as dataset’s size increases."
  },
  {
    "objectID": "tutorials/allknn.html#environment-and-dependencies",
    "href": "tutorials/allknn.html#environment-and-dependencies",
    "title": "Computing all \\(k\\) nearest neighbors",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "tutorials/basic-usage.html",
    "href": "tutorials/basic-usage.html",
    "title": "Using the SimilaritySearch package",
    "section": "",
    "text": "by: Eric S. Téllez\nusing SimilaritySearch, Markdown\nThis is a small tutorial showing a minimum example for working with SimilaritySearch it accepts several options that are let to defaults. While this should be enough for many purposes, you are invited to see the rest of the tutorials to take advantage of other features.\nMatrixDatabase is a required wrapper that tells SimilaritySearch how to access underlying objects since it can support different kinds of objects. In this setup, each column is an object and will be accessed through views using the MatrixDatabase. Since the backend doesn’t support appends or pushes, the index can be seen as an static index.\nfunction synthetic_benchmark(n, m, dim)\n    db = MatrixDatabase(randn(Float32, dim, n))\n    queries = MatrixDatabase(randn(Float32, dim, m))\n    dist = SqL2Distance()\n    (; db, queries, dist)\nend\nit can use any distance function described in SimilaritySearch and Distances.jl, and in fact any SemiMetric as described in the later package. The index construction is made as follows\nB = synthetic_benchmark(3000, 50, 2)\nG = SearchGraph(; B.dist, B.db)\nctx = SearchGraphContext()\nindex!(G, ctx)\nthis will display a lot of information in the console, since as construction advances the hyperparameters of the index are adjusted. The default optimization try to get a recall of 0.9 which is a typical tradeoff between quality and speed. Once the index is created, the index can solve nearest neighbor queries\nk = 16\n1I, D = searchbatch(G, ctx, B.queries, k)\n\n\n1\n\nThe searchbatch functions takes a set of queries and solve them using the given index. I is a matrix of identifiers in db and D their corresponding distances."
  },
  {
    "objectID": "tutorials/basic-usage.html#visualizing-what-we-just-did",
    "href": "tutorials/basic-usage.html#visualizing-what-we-just-did",
    "title": "Using the SimilaritySearch package",
    "section": "Visualizing what we just did",
    "text": "Visualizing what we just did\n\nusing Plots\n\nscatter(B.db.matrix[1, :], B.db.matrix[2, :], size=(600, 600), color=:cyan, ma=0.3, a=0.3, ms=1, msw=0, label=\"\")\nfor c in eachcol(I)\n    R = B.db.matrix[:, c]\n    @views scatter!(R[1, :], R[2, :], m=:diamond, ma=0.3, a=0.3, color=:auto, ms=2, msw=0, label=\"\")\nend\n\n@views scatter!(B.queries.matrix[1, :], B.queries.matrix[2, :], color=:black, m=:star, ma=0.5, a=0.5, ms=4, msw=0, label=\"\")\n\nplot!()\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCyan points identify the dataset while starts are query points. The nearest neighbor points are colored automatically and can repeat, but they come quite close to query points, in dense areas they are even hidding them."
  },
  {
    "objectID": "tutorials/basic-usage.html#environment-and-dependencies",
    "href": "tutorials/basic-usage.html#environment-and-dependencies",
    "title": "Using the SimilaritySearch package",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "tutorials/parallel-construction-and-search.html",
    "href": "tutorials/parallel-construction-and-search.html",
    "title": "Parallel construction and parallel search",
    "section": "",
    "text": "by: Eric S. Téllez\nusing SimilaritySearch\nSimilarity search on very large datasets and high-dimensional datasets require high computational resources. In this example we show how to arallelize both the construction and search to be able to handle this kind of databases.\ndim = 16\n1db = MatrixDatabase(randn(Float32, dim, 10^5))\n2queries = MatrixDatabase(randn(Float32, dim, 30))\n3dist = SqL2Distance()\n4ctx = SearchGraphContext(parallel_first_block=256, parallel_block=512)\n5G = SearchGraph(; dist, db)\n6index!(G, ctx)\n\n\n1\n\nA synthetic database of dimension 16 and \\(10^5\\) vectors.\n\n2\n\nA synthetic query set of \\(30\\) points.\n\n3\n\nThe distance function.\n\n4\n\nThe search context working with SearchGraph; a set of hyperparameters for the index.\n\n5\n\nThe index definition.\n\n6\n\nThe index construction.\nThe SearchGraph construction algorithm is incremental:\nThe parallel construction is made with index! or append_items!; for this matter these functions accept a parallel_block argument via the ctx context, that controls how many elements are inserted at once, i.e., looking for its nearest neighbors in parallel and connected also in parallel.\nAs in the sequential version, a minimum number of elements must exists to work, and therefore, the parallel_first_block argument can also be specified. By default, it is equal to parallel_block. The parallel_block argument should be set to at least the number of available threads, and perhaps multiplying it by a small constant is also a good approach.\nNote that you must not call push_item!, append_items!, or index! from several threads. The default algorithm will takes advantage of the available threads using a single call."
  },
  {
    "objectID": "tutorials/parallel-construction-and-search.html#searching",
    "href": "tutorials/parallel-construction-and-search.html#searching",
    "title": "Parallel construction and parallel search",
    "section": "Searching",
    "text": "Searching\nOnce the index is constructed, you can solve batches in parallel and also single queries. In contrast with append, these functions can be called in multithreading algorithms. However, you must pause the searching requests while perform insertions (parallel or sequential); mixing insertions and search produces an undefined behavior for search results.\n\nI, D = searchbatch(G, ctx, queries, 10)\n\nThreads.@threads for i in eachindex(queries)\n    p = search(G, ctx, queries[i], KnnResult(10))\n    res = p.res\n    \n    print(\"=== $i -- nearest neighbor:\")\n    println(res[1])\n    print(\"=== $i -- result set:\")\n    println(collect(res))\n    print(\"=== $i -- identifiers:\")\n    println(collect(IdView(res))) # do something with `res`\n    print(\"=== $i -- distances:\")\n    println(collect(DistView(res))) # do something with `res`\nend\n\n=== 30 -- nearest neighbor:=== 16 -- nearest neighbor:=== 13 -- nearest neighbor:=== 14 -- nearest neighbor:=== 2 -- nearest neighbor:=== 22 -- nearest neighbor:=== 11 -- nearest neighbor:=== 21 -- nearest neighbor:=== 25 -- nearest neighbor:=== 12 -- nearest neighbor:=== 19 -- nearest neighbor:=== 20 -- nearest neighbor:=== 15 -- nearest neighbor:=== 26 -- nearest neighbor:=== 5 -- nearest neighbor:=== 7 -- nearest neighbor:=== 27 -- nearest neighbor:=== 3 -- nearest neighbor:=== 18 -- nearest neighbor:=== 4 -- nearest neighbor:=== 29 -- nearest neighbor:=== 8 -- nearest neighbor:=== 28 -- nearest neighbor:=== 1 -- nearest neighbor:IdWeight(=== 9 -- nearest neighbor:=== 17 -- nearest neighbor:=== 6 -- nearest neighbor:=== 10 -- nearest neighbor:=== 24 -- nearest neighbor:=== 23 -- nearest neighbor:0x00011378, 5.7673936f0)\n=== 30 -- result set:IdWeight(0x000037f6, 4.1997943f0)\n=== 11 -- result set:IdWeight(0x00017724, 4.1756215f0)\n=== 20 -- result set:IdWeight(0x00007203, 5.063024f0)\n=== 21 -- result set:IdWeight(0x0000e327, 6.0607953f0)\n=== 28 -- result set:IdWeight(0x0000a963, 3.20485f0)\n=== 13 -- result set:IdWeight(0x00011b60, 3.7669861f0)\n=== 27 -- result set:IdWeight(0x00014acf, 6.6534595f0)\n=== 29 -- result set:IdWeight(0x00017dc9, 5.9772186f0)\n=== 7 -- result set:IdWeight(0x0000a2fa, 3.745455f0)\n=== 22 -- result set:IdWeight(0x0000b0cc, 1.3943706f0)\n=== 26 -- result set:IdWeight(0x0000b6b3, 4.0657415f0)\n=== 24 -- result set:IdWeight(0x0000b85e, 8.263314f0)\n=== 9 -- result set:IdWeight(0x00016624, 4.8849845f0)\n=== 23 -- result set:IdWeight(0x00000490, 5.165486f0)\n=== 5 -- result set:IdWeight(0x00005612, 6.526547f0)\n=== 4 -- result set:IdWeight(0x0001500b, 9.902931f0)\n=== 6 -- result set:IdWeight(0x000129b6, 7.12982f0)\nIdWeight=== 18 -- result set:(0x0000decd, 4.733655f0)\n=== 10 -- result set:IdWeight(0x000059d5, 3.6504257f0)\nIdWeight=== 1 -- result set:(0x000167fd, 6.492943f0)\n=== 12 -- result set:IdWeight(0x0000102c, 5.360827f0)\nIdWeight=== 8 -- result set:(0x00007707, 2.721734f0)\n=== 14 -- result set:IdWeight(0x000172e1, 6.1124606f0)\n=== 3 -- result set:IdWeight(0x0000187e, 4.252377f0)\nIdWeight=== 15 -- result set:(0x00002333, 4.3637156f0)\nIdWeight=== 17 -- result set:(0x000067eb, 4.072066f0)\n=== 25 -- result set:IdWeight(0x00009d5d, 1.5775208f0)\n=== 16 -- result set:IdWeight(0x0000f190, 4.866307f0)\n=== 19 -- result set:IdWeight(0x000022c6, 5.0453906f0)\n=== 2 -- result set:IdWeight[IdWeight(0x00011378, 5.7673936f0), IdWeight(0x0001211c, 6.5765514f0), IdWeight(0x00008555, 6.8664637f0), IdWeight(0x00006b58, 6.9316936f0), IdWeight(0x00008435, 7.555251f0), IdWeight(0x0000ee2f, 7.884663f0), IdWeight(0x00002f89, 8.138638f0), IdWeight(0x00001804, 8.208462f0), IdWeight(0x00017847, 8.320134f0), IdWeight(0x00009d65, 8.574164f0)]\n=== 30 -- identifiers:IdWeight[IdWeight(0x000022c6, 5.0453906f0), IdWeight(0x0000f3b4, 5.2503996f0), IdWeight(0x00011748, 6.7221427f0), IdWeight(0x000003fa, 6.816469f0), IdWeight(0x00003abc, 7.6534805f0), IdWeight(0x0000c833, 7.8723927f0), IdWeight(0x00008c81, 7.9628296f0), IdWeight(0x00001b9c, 8.2568445f0), IdWeight(0x00000959, 8.341556f0), IdWeight(0x00014830, 8.578199f0)]\n=== 2 -- identifiers:IdWeight[IdWeight(0x000059d5, 3.6504257f0), IdWeight(0x00016325, 4.4379272f0), IdWeight(0x0000041b, 4.7488112f0), IdWeight(0x00015981, 4.901279f0), IdWeight(0x0001865e, 4.9625344f0), IdWeight(0x00014c7c, 5.3858833f0), IdWeight(0x00010104, 5.4063106f0), IdWeight(0x0000e6d8, 5.5065255f0), IdWeight(0x000133c3, 5.513593f0), IdWeight(0x0000d08f, 5.6105084f0)]\n=== 1 -- identifiers:IdWeight[IdWeight(0x00002333, 4.3637156f0), IdWeight(0x0000aa93, 5.230916f0), IdWeight(0x00000bbb, 5.778029f0), IdWeight(0x0000d574, 5.791041f0), IdWeight(0x00003ce7, 5.871272f0), IdWeight(0x000023e4, 6.152666f0), IdWeight(0x0000eb0e, 6.282268f0), IdWeight(0x00010489, 6.393247f0), IdWeight(0x0000bd04, 6.39784f0), IdWeight(0x0000bae3, 6.6380563f0)]\n=== 17 -- identifiers:IdWeight[IdWeight(0x00000490, 5.165486f0), IdWeight(0x000160d9, 5.416849f0), IdWeight(0x0001675c, 5.754625f0), IdWeight(0x00001106, 6.0301943f0), IdWeight(0x00017ce0, 6.540473f0), IdWeight(0x00012f04, 6.7363224f0), IdWeight(0x0000b758, 7.0457015f0), IdWeight(0x0000d8e6, 7.2509794f0), IdWeight(0x00005098, 7.5190277f0), IdWeight(0x00004d55, 7.736002f0)]\n=== 5 -- identifiers:IdWeight[IdWeight(0x0000b6b3, 4.0657415f0), IdWeight(0x0001800e, 4.1596627f0), IdWeight(0x000130c8, 5.215902f0), IdWeight(0x00007d21, 5.2420435f0), IdWeight(0x00016a6d, 5.401593f0), IdWeight(0x00000ba3, 5.982858f0), IdWeight(0x000178fb, 6.190426f0), IdWeight(0x000015d3, 6.3417892f0), IdWeight(0x000087ed, 6.4037395f0), IdWeight(0x00005ddd, 6.57997f0)]\n=== 24 -- identifiers:IdWeight[IdWeight(0x000037f6, 4.1997943f0), IdWeight(0x00017ea5, 4.2164073f0), IdWeight(0x00007944, 4.498477f0), IdWeight(0x0000c349, 4.663265f0), IdWeight(0x0000a46c, 4.7884846f0), IdWeight(0x00016aae, 4.8613787f0), IdWeight(0x00010e4d, 4.95706f0), IdWeight(0x000115dd, 5.082154f0), IdWeight(0x00004eea, 5.189611f0), IdWeight(0x0000be17, 5.2598653f0)]\n=== 11 -- identifiers:IdWeight[IdWeight(0x000129b6, 7.12982f0), IdWeight(0x00015c1b, 7.77421f0), IdWeight(0x00007e5e, 7.8367004f0), IdWeight(0x00002153, 8.171021f0), IdWeight(0x0000626d, 8.411317f0), IdWeight(0x00000b3f, 8.664575f0), IdWeight(0x0000aecb, 8.808345f0), IdWeight(0x000092a8, 9.082102f0), IdWeight(0x0000d793, 9.114776f0), IdWeight(0x0000f742, 9.706217f0)]\n=== 18 -- identifiers:IdWeight[IdWeight(0x000167fd, 6.492943f0), IdWeight(0x0000a032, 7.065445f0), IdWeight(0x000114c4, 7.4024973f0), IdWeight(0x0000071b, 7.550003f0), IdWeight(0x00008c6e, 8.070904f0), IdWeight(0x0000a8de, 8.096019f0), IdWeight(0x00010e55, 9.163538f0), IdWeight(0x0000ad53, 9.244366f0), IdWeight(0x00000cae, 9.499945f0), IdWeight(0x00013970, 9.588688f0)]\n=== 12 -- identifiers:IdWeight[IdWeight(0x00005612, 6.526547f0), IdWeight(0x0000a3ea, 7.7701325f0), IdWeight(0x0000d3f7, 8.0697365f0), IdWeight(0x0000c18a, 8.142321f0), IdWeight(0x00009780, 8.222589f0), IdWeight(0x0000745a, 8.274956f0), IdWeight(0x0000bd07, 8.325199f0), IdWeight(0x0000abfa, 8.353053f0), IdWeight(0x0000f3e4, 8.623434f0), IdWeight(0x00013959, 8.665885f0)]\n=== 4 -- identifiers:IdWeight[IdWeight(0x0000102c, 5.360827f0), IdWeight(0x0000a830, 5.979519f0), IdWeight(0x00007096, 6.0358853f0), IdWeight(0x000074ce, 6.0892773f0), IdWeight(0x0000f85b, 6.0987215f0), IdWeight(0x0001649b, 6.139133f0), IdWeight(0x00017b71, 6.182934f0), IdWeight(0x0000e86d, 6.305562f0), IdWeight(0x00003994, 6.566434f0), IdWeight(0x0001689d, 6.7839475f0)]\n=== 8 -- identifiers:IdWeight[IdWeight(0x00011b60, 3.7669861f0), IdWeight(0x0000b250, 4.023312f0), IdWeight(0x000129a7, 4.153792f0), IdWeight(0x0000f05e, 4.353279f0), IdWeight(0x00012a5b, 4.8698397f0), IdWeight(0x00010a91, 5.1262593f0), IdWeight(0x0000bea3, 5.26887f0), IdWeight(0x00016966, 5.394973f0), IdWeight(0x0000d46f, 5.4349174f0), IdWeight(0x00013a36, 5.654727f0)]\n=== 27 -- identifiers:IdWeight[IdWeight(0x000172e1, 6.1124606f0), IdWeight(0x0000440b, 6.3649597f0), IdWeight(0x000086c5, 6.5696106f0), IdWeight(0x00011034, 7.120795f0), IdWeight(0x0000e97c, 7.4672546f0), IdWeight(0x0000de52, 7.480347f0), IdWeight(0x00017173, 7.608932f0), IdWeight(0x00003f45, 7.674545f0), IdWeight(0x0000ccfd, 7.7937264f0), IdWeight(0x000087f5, 8.198435f0)]\n=== 3 -- identifiers:IdWeight[IdWeight(0x00016624, 4.8849845f0), IdWeight(0x0000f33c, 5.949905f0), IdWeight(0x0000f5ca, 6.370414f0), IdWeight(0x000068b8, 6.4893875f0), IdWeight(0x00011aa3, 7.216407f0), IdWeight(0x000178a0, 7.4125123f0), IdWeight(0x0000b1ea, 7.474535f0), IdWeight(0x00017f97, 7.5206814f0), IdWeight(0x0000054a, 7.561662f0), IdWeight(0x00003ff0, 7.658282f0)]\n=== 23 -- identifiers:IdWeight[IdWeight(0x0000b0cc, 1.3943706f0), IdWeight(0x0001550f, 1.7497267f0), IdWeight(0x000155f3, 2.4860287f0), IdWeight(0x0000297f, 2.5835958f0), IdWeight(0x0000171c, 2.829751f0), IdWeight(0x00004769, 3.0909886f0), IdWeight(0x00016916, 3.2841733f0), IdWeight(0x00013748, 3.3177545f0), IdWeight(0x000184ba, 3.3480034f0), IdWeight(0x00007a42, 3.3543985f0)]\n=== 26 -- identifiers:IdWeight[IdWeight(0x00009d5d, 1.5775208f0), IdWeight(0x0000e105, 4.171344f0), IdWeight(0x0000da0b, 4.5726433f0), IdWeight(0x00004549, 4.749284f0), IdWeight(0x00008308, 4.7750363f0), IdWeight(0x00014864, 4.861708f0), IdWeight(0x00006bdc, 4.9597464f0), IdWeight(0x0000284d, 4.9875546f0), IdWeight(0x00012b9a, 5.248585f0), IdWeight(0x000056de, 5.3048162f0)]\n=== 16 -- identifiers:IdWeight[IdWeight(0x00017724, 4.1756215f0), IdWeight(0x00013cf6, 4.2562156f0), IdWeight(0x000172fa, 4.274527f0), IdWeight(0x00012437, 4.3392167f0), IdWeight(0x00014072, 4.5360665f0), IdWeight(0x000024c5, 4.6235723f0), IdWeight(0x0001073b, 4.7087693f0), IdWeight(0x00008e2d, 4.794657f0), IdWeight(0x0000b966, 4.950298f0), IdWeight(0x0000f550, 5.162191f0)]\n=== 20 -- identifiers:IdWeight[IdWeight(0x0000f190, 4.866307f0), IdWeight(0x00012f95, 4.8982434f0), IdWeight(0x00009730, 5.331491f0), IdWeight(0x000160a1, 5.4492016f0), IdWeight(0x0000387d, 5.60643f0), IdWeight(0x0000cfa6, 5.655481f0), IdWeight(0x000026cf, 5.746117f0), IdWeight(0x00003502, 5.8292413f0), IdWeight(0x0000b77a, 6.044111f0), IdWeight(0x0000d2ee, 6.238947f0)]\n=== 19 -- identifiers:IdWeight[IdWeight(0x00007203, 5.063024f0), IdWeight(0x0000c46a, 5.101182f0), IdWeight(0x00002fe7, 5.136405f0), IdWeight(0x0000fd21, 5.4088683f0), IdWeight(0x00003d5f, 5.650648f0), IdWeight(0x000025b6, 5.718283f0), IdWeight(0x0000373e, 5.9018197f0), IdWeight(0x00010843, 5.9279447f0), IdWeight(0x0001043b, 5.940809f0), IdWeight(0x00005eee, 5.953355f0)]\n=== 21 -- identifiers:IdWeight[IdWeight(0x0000e327, 6.0607953f0), IdWeight(0x00010ce8, 6.980864f0), IdWeight(0x0000f0b9, 7.044319f0), IdWeight(0x00014d3b, 7.5088387f0), IdWeight(0x000120bf, 8.238833f0), IdWeight(0x0000402b, 8.332623f0), IdWeight(0x0000464a, 8.460111f0), IdWeight(0x00003402, 8.462259f0), IdWeight(0x00006356, 8.469164f0), IdWeight(0x0000384c, 8.582058f0)]\n=== 28 -- identifiers:IdWeight[IdWeight(0x00014acf, 6.6534595f0), IdWeight(0x00003e02, 7.996436f0), IdWeight(0x000086b1, 8.9359455f0), IdWeight(0x0001795c, 9.036238f0), IdWeight(0x00008110, 9.3581505f0), IdWeight(0x0001845d, 9.501015f0), IdWeight(0x00000f45, 9.701177f0), IdWeight(0x00007ee0, 9.780923f0), IdWeight(0x0000e77e, 10.008263f0), IdWeight(0x0001685a, 10.073041f0)]\n=== 29 -- identifiers:IdWeight[IdWeight(0x0000decd, 4.733655f0), IdWeight(0x000032e6, 5.426944f0), IdWeight(0x00011439, 6.0009217f0), IdWeight(0x000102a0, 6.1232796f0), IdWeight(0x00014432, 6.220991f0), IdWeight(0x00001187, 6.436433f0), IdWeight(0x00017af4, 6.4974957f0), IdWeight(0x00001322, 6.8033752f0), IdWeight(0x000140f8, 6.8596373f0), IdWeight(0x00010347, 6.884148f0)]\n=== 10 -- identifiers:IdWeight[IdWeight(0x0000a2fa, 3.745455f0), IdWeight(0x000166d7, 4.1904883f0), IdWeight(0x0000daea, 4.360644f0), IdWeight(0x00011653, 4.4343615f0), IdWeight(0x0000057e, 4.5342555f0), IdWeight(0x000120fe, 4.6888514f0), IdWeight(0x00016d41, 5.1771836f0), IdWeight(0x000005b1, 5.1967874f0), IdWeight(0x0000da3b, 5.2599225f0), IdWeight(0x00009e25, 5.267457f0)]\n=== 22 -- identifiers:IdWeight[IdWeight(0x0001500b, 9.902931f0), IdWeight(0x00009216, 10.881981f0), IdWeight(0x0000fb06, 11.389378f0), IdWeight(0x00016736, 12.514006f0), IdWeight(0x0000b50f, 12.59656f0), IdWeight(0x00011719, 12.77947f0), IdWeight(0x00013715, 13.215257f0), IdWeight(0x00004a59, 13.24985f0), IdWeight(0x000110a4, 13.296212f0), IdWeight(0x00009365, 13.34092f0)]\n=== 6 -- identifiers:IdWeight[IdWeight(0x00017dc9, 5.9772186f0), IdWeight(0x00008e12, 6.214102f0), IdWeight(0x00007934, 6.2443905f0), IdWeight(0x000053fe, 6.490207f0), IdWeight(0x000113a2, 6.534569f0), IdWeight(0x000176ac, 6.7535625f0), IdWeight(0x00016c95, 6.7655654f0), IdWeight(0x00007f85, 6.8923063f0), IdWeight(0x000139a5, 6.9544535f0), IdWeight(0x0001135c, 6.9556313f0)]\n=== 7 -- identifiers:IdWeight[IdWeight(0x0000187e, 4.252377f0), IdWeight(0x000141f5, 4.8947763f0), IdWeight(0x00018414, 4.9112787f0), IdWeight(0x00012b47, 5.0469704f0), IdWeight(0x0000bdd4, 5.2809973f0), IdWeight(0x000156bc, 5.324951f0), IdWeight(0x0000ccdd, 5.4074326f0), IdWeight(0x0000c503, 5.4483457f0), IdWeight(0x00005127, 5.4923935f0), IdWeight(0x00006c32, 5.6581397f0)]\n=== 15 -- identifiers:IdWeight[IdWeight(0x000067eb, 4.072066f0), IdWeight(0x00006e17, 5.7219296f0), IdWeight(0x000011f2, 5.974982f0), IdWeight(0x00004ca1, 6.3699007f0), IdWeight(0x00013235, 6.5592103f0), IdWeight(0x0000f8dd, 6.780655f0), IdWeight(0x000101e9, 6.9018593f0), IdWeight(0x000185ee, 6.952215f0), IdWeight(0x00014372, 6.975828f0), IdWeight(0x00018284, 7.316987f0)]\n=== 25 -- identifiers:IdWeight[IdWeight(0x0000a963, 3.20485f0), IdWeight(0x00012d45, 3.9892905f0), IdWeight(0x000169b0, 4.1061463f0), IdWeight(0x00011a32, 4.4000716f0), IdWeight(0x0001048a, 4.40868f0), IdWeight(0x00006359, 4.7227683f0), IdWeight(0x0001780d, 4.8818636f0), IdWeight(0x00016e8f, 5.0930586f0), IdWeight(0x00010fbb, 5.1149693f0), IdWeight(0x0000a6c7, 5.2317815f0)]\n=== 13 -- identifiers:UInt32[0x00009d5d, 0x0000e105, 0x0000da0b, 0x00004549, 0x00008308, 0x00014864, 0x00006bdc, 0x0000284d, 0x00012b9a, 0x000056de]\n=== 16 -- distances:UInt32[0x00011b60, 0x0000b250, 0x000129a7, 0x0000f05e, 0x00012a5b, 0x00010a91, 0x0000bea3, 0x00016966, 0x0000d46f, 0x00013a36]\n=== 27 -- distances:UInt32[0x00011378, 0x0001211c, 0x00008555, 0x00006b58, 0x00008435, 0x0000ee2f, 0x00002f89, 0x00001804, 0x00017847, 0x00009d65]\n=== 30 -- distances:UInt32[0x000067eb, 0x00006e17, 0x000011f2, 0x00004ca1, 0x00013235, 0x0000f8dd, 0x000101e9, 0x000185ee, 0x00014372, 0x00018284]\n=== 25 -- distances:UInt32[0x0000a2fa, 0x000166d7, 0x0000daea, 0x00011653, 0x0000057e, 0x000120fe, 0x00016d41, 0x000005b1, 0x0000da3b, 0x00009e25]\n=== 22 -- distances:UInt32[0x0001500b, 0x00009216, 0x0000fb06, 0x00016736, 0x0000b50f, 0x00011719, 0x00013715, 0x00004a59, 0x000110a4, 0x00009365]\n=== 6 -- distances:UInt32[0x000172e1, 0x0000440b, 0x000086c5, 0x00011034, 0x0000e97c, 0x0000de52, 0x00017173, 0x00003f45, 0x0000ccfd, 0x000087f5]\n=== 3 -- distances:UInt32[0x000167fd, 0x0000a032, 0x000114c4, 0x0000071b, 0x00008c6e, 0x0000a8de, 0x00010e55, 0x0000ad53, 0x00000cae, 0x00013970]\n=== 12 -- distances:UInt32[0x00017dc9, 0x00008e12, 0x00007934, 0x000053fe, 0x000113a2, 0x000176ac, 0x00016c95, 0x00007f85, 0x000139a5, 0x0001135c]\n=== 7 -- distances:UInt32[0x00007203, 0x0000c46a, 0x00002fe7, 0x0000fd21, 0x00003d5f, 0x000025b6, 0x0000373e, 0x00010843, 0x0001043b, 0x00005eee]\n=== 21 -- distances:UInt32[0x0000a963, 0x00012d45, 0x000169b0, 0x00011a32, 0x0001048a, 0x00006359, 0x0001780d, 0x00016e8f, 0x00010fbb, 0x0000a6c7]\n=== 13 -- distances:UInt32[0x00016624, 0x0000f33c, 0x0000f5ca, 0x000068b8, 0x00011aa3, 0x000178a0, 0x0000b1ea, 0x00017f97, 0x0000054a, 0x00003ff0]\n=== 23 -- distances:UInt32[0x00017724, 0x00013cf6, 0x000172fa, 0x00012437, 0x00014072, 0x000024c5, 0x0001073b, 0x00008e2d, 0x0000b966, 0x0000f550]\n=== 20 -- distances:UInt32[0x00005612, 0x0000a3ea, 0x0000d3f7, 0x0000c18a, 0x00009780, 0x0000745a, 0x0000bd07, 0x0000abfa, 0x0000f3e4, 0x00013959]\n=== 4 -- distances:UInt32[0x00000490, 0x000160d9, 0x0001675c, 0x00001106, 0x00017ce0, 0x00012f04, 0x0000b758, 0x0000d8e6, 0x00005098, 0x00004d55]\n=== 5 -- distances:UInt32[0x00014acf, 0x00003e02, 0x000086b1, 0x0001795c, 0x00008110, 0x0001845d, 0x00000f45, 0x00007ee0, 0x0000e77e, 0x0001685a]\n=== 29 -- distances:UInt32[0x000037f6, 0x00017ea5, 0x00007944, 0x0000c349, 0x0000a46c, 0x00016aae, 0x00010e4d, 0x000115dd, 0x00004eea, 0x0000be17]\n=== 11 -- distances:UInt32[0x0000102c, 0x0000a830, 0x00007096, 0x000074ce, 0x0000f85b, 0x0001649b, 0x00017b71, 0x0000e86d, 0x00003994, 0x0001689d]\n=== 8 -- distances:UInt32[0x000059d5, 0x00016325, 0x0000041b, 0x00015981, 0x0001865e, 0x00014c7c, 0x00010104, 0x0000e6d8, 0x000133c3, 0x0000d08f]\n=== 1 -- distances:UInt32[0x0000187e, 0x000141f5, 0x00018414, 0x00012b47, 0x0000bdd4, 0x000156bc, 0x0000ccdd, 0x0000c503, 0x00005127, 0x00006c32]\n=== 15 -- distances:UInt32[0x00002333, 0x0000aa93, 0x00000bbb, 0x0000d574, 0x00003ce7, 0x000023e4, 0x0000eb0e, 0x00010489, 0x0000bd04, 0x0000bae3]\n=== 17 -- distances:UInt32[0x0000b6b3, 0x0001800e, 0x000130c8, 0x00007d21, 0x00016a6d, 0x00000ba3, 0x000178fb, 0x000015d3, 0x000087ed, 0x00005ddd]\n=== 24 -- distances:UInt32[0x0000decd, 0x000032e6, 0x00011439, 0x000102a0, 0x00014432, 0x00001187, 0x00017af4, 0x00001322, 0x000140f8, 0x00010347]\n=== 10 -- distances:UInt32[0x0000f190, 0x00012f95, 0x00009730, 0x000160a1, 0x0000387d, 0x0000cfa6, 0x000026cf, 0x00003502, 0x0000b77a, 0x0000d2ee]\n=== 19 -- distances:UInt32[0x000129b6, 0x00015c1b, 0x00007e5e, 0x00002153, 0x0000626d, 0x00000b3f, 0x0000aecb, 0x000092a8, 0x0000d793, 0x0000f742]\n=== 18 -- distances:UInt32[0x0000b0cc, 0x0001550f, 0x000155f3, 0x0000297f, 0x0000171c, 0x00004769, 0x00016916, 0x00013748, 0x000184ba, 0x00007a42]\n=== 26 -- distances:IdWeight[IdWeight(0x0000b85e, 8.263314f0), IdWeight(0x000159d7, 8.982347f0), IdWeight(0x00001dc2, 9.312948f0), IdWeight(0x0000ed4f, 10.208364f0), IdWeight(0x0000804e, 10.458137f0), IdWeight(0x00012803, 10.493764f0), IdWeight(0x00015c3b, 10.561956f0), IdWeight(0x00007d6b, 10.588476f0), IdWeight(0x0000f888, 10.703102f0), IdWeight(0x0000fccc, 10.796254f0)]\n=== 9 -- identifiers:UInt32[0x0000e327, 0x00010ce8, 0x0000f0b9, 0x00014d3b, 0x000120bf, 0x0000402b, 0x0000464a, 0x00003402, 0x00006356, 0x0000384c]\n=== 28 -- distances:UInt32[0x000022c6, 0x0000f3b4, 0x00011748, 0x000003fa, 0x00003abc, 0x0000c833, 0x00008c81, 0x00001b9c, 0x00000959, 0x00014830]\n=== 2 -- distances:UInt32[0x0000b85e, 0x000159d7, 0x00001dc2, 0x0000ed4f, 0x0000804e, 0x00012803, 0x00015c3b, 0x00007d6b, 0x0000f888, 0x0000fccc]\n=== 9 -- distances:IdWeight[IdWeight(0x00007707, 2.721734f0), IdWeight(0x000118a1, 2.8061082f0), IdWeight(0x0000abe1, 3.2777224f0), IdWeight(0x00004f89, 3.4322693f0), IdWeight(0x0000325b, 3.7450671f0), IdWeight(0x000126e5, 3.8742676f0), IdWeight(0x00004a35, 3.8878646f0), IdWeight(0x000155d7, 3.9613428f0), IdWeight(0x00003fe3, 3.9672658f0), IdWeight(0x000011fa, 4.060509f0)]\n=== 14 -- identifiers:UInt32[0x00007707, 0x000118a1, 0x0000abe1, 0x00004f89, 0x0000325b, 0x000126e5, 0x00004a35, 0x000155d7, 0x00003fe3, 0x000011fa]\n=== 14 -- distances:Float32[1.5775208, 4.171344, 4.5726433, 4.749284, 4.7750363, 4.861708, 4.9597464, 4.9875546, 5.248585, 5.3048162]\nFloat32[4.1756215, 4.2562156, 4.274527, 4.3392167, 4.5360665, 4.6235723, 4.7087693, 4.794657, 4.950298, 5.162191]\nFloat32[5.063024, 5.101182, 5.136405, 5.4088683, 5.650648, 5.718283, 5.9018197, 5.9279447, 5.940809, 5.953355]\nFloat32[3.6504257, 4.4379272, 4.7488112, 4.901279, 4.9625344, 5.3858833, 5.4063106, 5.5065255, 5.513593, 5.6105084]\nFloat32[5.165486, 5.416849, 5.754625, 6.0301943, 6.540473, 6.7363224, 7.0457015, 7.2509794, 7.5190277, 7.736002]\nFloat32[4.1997943, 4.2164073, 4.498477, 4.663265, 4.7884846, 4.8613787, 4.95706, 5.082154, 5.189611, 5.2598653]\nFloat32[4.8849845, 5.949905, 6.370414, 6.4893875, 7.216407, 7.4125123, 7.474535, 7.5206814, 7.561662, 7.658282]\nFloat32[6.526547, 7.7701325, 8.0697365, 8.142321, 8.222589, 8.274956, 8.325199, 8.353053, 8.623434, 8.665885]\nFloat32[4.072066, 5.7219296, 5.974982, 6.3699007, 6.5592103, 6.780655, 6.9018593, 6.952215, 6.975828, 7.316987]\nFloat32[7.12982, 7.77421, 7.8367004, 8.171021, 8.411317, 8.664575, 8.808345, 9.082102, 9.114776, 9.706217]\nFloat32[4.3637156, 5.230916, 5.778029, 5.791041, 5.871272, 6.152666, 6.282268, 6.393247, 6.39784, 6.6380563]\nFloat32[6.0607953, 6.980864, 7.044319, 7.5088387, 8.238833, 8.332623, 8.460111, 8.462259, 8.469164, 8.582058]\nFloat32[5.7673936, 6.5765514, 6.8664637, 6.9316936, 7.555251, 7.884663, 8.138638, 8.208462, 8.320134, 8.574164]\nFloat32[3.745455, 4.1904883, 4.360644, 4.4343615, 4.5342555, 4.6888514, 5.1771836, 5.1967874, 5.2599225, 5.267457]\nFloat32[6.492943, 7.065445, 7.4024973, 7.550003, 8.070904, 8.096019, 9.163538, 9.244366, 9.499945, 9.588688]\nFloat32[8.263314, 8.982347, 9.312948, 10.208364, 10.458137, 10.493764, 10.561956, 10.588476, 10.703102, 10.796254]\nFloat32[1.3943706, 1.7497267, 2.4860287, 2.5835958, 2.829751, 3.0909886, 3.2841733, 3.3177545, 3.3480034, 3.3543985]\nFloat32[5.9772186, 6.214102, 6.2443905, 6.490207, 6.534569, 6.7535625, 6.7655654, 6.8923063, 6.9544535, 6.9556313]\nFloat32[6.1124606, 6.3649597, 6.5696106, 7.120795, 7.4672546, 7.480347, 7.608932, 7.674545, 7.7937264, 8.198435]\nFloat32[4.0657415, 4.1596627, 5.215902, 5.2420435, 5.401593, 5.982858, 6.190426, 6.3417892, 6.4037395, 6.57997]\nFloat32[3.20485, 3.9892905, 4.1061463, 4.4000716, 4.40868, 4.7227683, 4.8818636, 5.0930586, 5.1149693, 5.2317815]\nFloat32[6.6534595, 7.996436, 8.9359455, 9.036238, 9.3581505, 9.501015, 9.701177, 9.780923, 10.008263, 10.073041]\nFloat32[4.733655, 5.426944, 6.0009217, 6.1232796, 6.220991, 6.436433, 6.4974957, 6.8033752, 6.8596373, 6.884148]\nFloat32[3.7669861, 4.023312, 4.153792, 4.353279, 4.8698397, 5.1262593, 5.26887, 5.394973, 5.4349174, 5.654727]\nFloat32[9.902931, 10.881981, 11.389378, 12.514006, 12.59656, 12.77947, 13.215257, 13.24985, 13.296212, 13.34092]\nFloat32[5.0453906, 5.2503996, 6.7221427, 6.816469, 7.6534805, 7.8723927, 7.9628296, 8.2568445, 8.341556, 8.578199]\nFloat32[5.360827, 5.979519, 6.0358853, 6.0892773, 6.0987215, 6.139133, 6.182934, 6.305562, 6.566434, 6.7839475]\nFloat32[4.866307, 4.8982434, 5.331491, 5.4492016, 5.60643, 5.655481, 5.746117, 5.8292413, 6.044111, 6.238947]\nFloat32[2.721734, 2.8061082, 3.2777224, 3.4322693, 3.7450671, 3.8742676, 3.8878646, 3.9613428, 3.9672658, 4.060509]\nFloat32[4.252377, 4.8947763, 4.9112787, 5.0469704, 5.2809973, 5.324951, 5.4074326, 5.4483457, 5.4923935, 5.6581397]"
  },
  {
    "objectID": "tutorials/parallel-construction-and-search.html#environment-and-dependencies",
    "href": "tutorials/parallel-construction-and-search.html#environment-and-dependencies",
    "title": "Parallel construction and parallel search",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "demos/primes.html",
    "href": "demos/primes.html",
    "title": "Prime numbers",
    "section": "",
    "text": "by: Eric S. Téllez\nThis demonstration is about prime numbers and its similarity based on its factors. This is a well-known demonstration of SimSearchManifoldLearning.jl. This notebook does not requires to download any dataset.\nNote: This example needs a lot of computing power; therefore you may want to set the environment variable JULIA_NUM_THREADS=auto before running julia.\nusing SimilaritySearch, SimSearchManifoldLearning, Primes, Plots, StatsBase, LinearAlgebra, Markdown, Random\nNow, we can define our dataset. The set of factors are found using the Primes package. Note that we use VectorDatabase to represent the dataset.\nn = 100_000\nF = Vector{Vector{Int32}}(undef, n+1)\n\nfunction encode_factors(num)\n    sort!(Int32[convert(Int32, f) for f in factor(Set, num)])\nend\n\nF[1] = Int32[1]\n\nfor i in 2:n+1\n    s = encode_factors(i)\n    F[i] = s\nend\n\ndb = VectorDatabase(F)\n#dist = JaccardDistance()  # Other distances from SimilaritySearch\n#dist = IntersectionDissimilarity()\n#dist = CosineDistanceSet()\ndist = DiceDistance()\nWe use Int32 ordered arrays to store prime factors to represent each integer. In the following cell define the cosine distance equivalent for this representation. While other representations may perform faster, this is quite straighforward and demonstrates the use of user’s defined distance functions."
  },
  {
    "objectID": "demos/primes.html#index-construction",
    "href": "demos/primes.html#index-construction",
    "title": "Prime numbers",
    "section": "Index construction",
    "text": "Index construction\nNote that the primes factors are pretty large for some large \\(n\\) and this imply challengues for metric indexes (since it is related with the intrinsic dimension of the dataset). We used a kernel that starts 64 threads, it solves \\(100000\\) in a few seconds but it can take pretty large time using single threads and larger \\(n\\) values. The construction of the index is used by the visualization algorithm (UMAP) to construct an all-knn graph, which can be a quite costly procedure.\n\nG = SearchGraph(; db, dist)\n1ctx = SearchGraphContext(hyperparameters_callback=OptimizeParameters(MinRecall(0.95)))\n2index!(G, ctx)\n3optimize_index!(G, ctx, MinRecall(0.9))\n\n\n1\n\nDefines the index and the search context (caches and hyperparameters); particularly, we use a very high quality build MinRecall(0.95); high quality constructions yield to faster queries due to the underlying graph structure.\n\n2\n\nActual indexing procedure using the given search context.\n\n3\n\nOptimizing the index to trade quality and speed."
  },
  {
    "objectID": "demos/primes.html#visualizing-with-umap-projection",
    "href": "demos/primes.html#visualizing-with-umap-projection",
    "title": "Prime numbers",
    "section": "Visualizing with UMAP projection",
    "text": "Visualizing with UMAP projection\nWe select to initialize the embedding randomly, this could yield to low quality embeddings, but it is much faster than other techniques like spectral layout. Note that we pass the Search graph G. We also use a second call to compute a 3D embedding for computing a kind of colour embedding, here we pass U2 to avoid recomputing several of the involved structures.\n\nfunction normcolors(V)\n    min_, max_ = extrema(V)\n    V .= (V .- min_) ./ (max_ - min_)\n    V .= clamp.(V, 0, 1)\nend\n\nnormcolors(@view e3[1, :])\nnormcolors(@view e3[2, :])\nnormcolors(@view e3[3, :])\n\nlet C = [RGB(c[1], c[2], c[3]) for c in eachcol(e3)],\n    X = view(e2, 1, :),\n    Y = view(e2, 2, :)\n    scatter(X, Y, color=C, fmt=:png, alpha=0.2, size=(600, 600), ma=0.3, ms=2, msw=0, label=\"\", yticks=nothing, xticks=nothing, xaxis=false, yaxis=false)\nend\n\nplot!()\n\n\n\n\n\ne2, e3 = let min_dist=0.5f0,\n             k=20,\n             n_epochs=75,\n             neg_sample_rate=3,\n             tol=1e-3,\n             layout=SpectralLayout()\n\n    @time \"Compute 2D UMAP model\" U2 = fit(UMAP, G; k, neg_sample_rate, layout, n_epochs, tol, min_dist)\n    @time \"Compute 3D UMAP model\" U3 = fit(U2, 3; neg_sample_rate, n_epochs, tol)\n    @time \"predicting 2D embeddings\" e2 = clamp.(predict(U2), -10f0, 10f0)\n    @time \"predicting 3D embeddings\" e3 = clamp.(predict(U3), -10f0, 10f0)\n    e2, e3\nend"
  },
  {
    "objectID": "demos/primes.html#final-notes",
    "href": "demos/primes.html#final-notes",
    "title": "Prime numbers",
    "section": "Final notes",
    "text": "Final notes\nThis example shows how to index and search dense vector databases, in particular GloVe word embeddings using the cosine distance. Low dimensional projections are made with SimSearchManifoldLearning, note that SimilaritySearch is also used for computing the all \\(k\\) nearest neighbors needed by the UMAP model. Note that this notebook should be ran with several threads to reduce time costs."
  },
  {
    "objectID": "demos/primes.html#environment-and-dependencies",
    "href": "demos/primes.html#environment-and-dependencies",
    "title": "Prime numbers",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n  [06eb3307] ManifoldLearning v0.9.0\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [27ebfcd6] Primes v0.5.7\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "demos/index.html",
    "href": "demos/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "We separate the examples by the kind of data, since some of the datasets are quite large and will require a lot of computer power. We also list how to connect SimilaritySearch with other packages that require solving k nearest neighbor queries.\n\n\n\n2d example\n\n\n\n\n\nPrime factors\nManifoldLearning – scurve and prime gaps\n\n\n\n\n\nGlove embeddings\nEmoji space\n\n\n\n\n\nMNIST"
  },
  {
    "objectID": "demos/index.html#list-of-examples",
    "href": "demos/index.html#list-of-examples",
    "title": "Tutorials",
    "section": "",
    "text": "We separate the examples by the kind of data, since some of the datasets are quite large and will require a lot of computer power. We also list how to connect SimilaritySearch with other packages that require solving k nearest neighbor queries.\n\n\n\n2d example\n\n\n\n\n\nPrime factors\nManifoldLearning – scurve and prime gaps\n\n\n\n\n\nGlove embeddings\nEmoji space\n\n\n\n\n\nMNIST"
  },
  {
    "objectID": "demos/index.html#search-demos-and-umap-visualization",
    "href": "demos/index.html#search-demos-and-umap-visualization",
    "title": "Tutorials",
    "section": "Search demos and UMAP visualization",
    "text": "Search demos and UMAP visualization\nThe demos are Pluto and [Jupyter](https://jupyter.org/] notebooks. Inside the repo’s root run the following commands.\n\n$ JULIA_NUM_THREADS=auto julia --project=.\n...\n\njulia&gt; using Pluto\n...\njulia&gt; Pluto.run(notebook=\"WIT/wit-demo.jl\")\n...\nor\n\n$ JULIA_NUM_THREADS=auto jupyter-lab .\nPlease recall that the first time you load a package Julia compiles it. Pluto notebooks also save its own environments and therefore it can use different package versions that those listed in the repo environment, which will cause installing and compiling packages the first time the notebooks run. Hopefully, this strategy improves the reproducibility at the cost of increasing loading times. Jupyter notebooks also contain the necessary package-manager instructions to improve reproducibility.\nNote: Pluto interface also allows loading notebooks, so you don’t need to exit and re-run to explore examples.\n\nVisualization\nMost visualizations are made with UMAP models using the SimSearchManifoldLearning package. These can be expensive and it is always recommended to run notebooks with all available threads."
  },
  {
    "objectID": "demos/index.html#initializing-the-environment",
    "href": "demos/index.html#initializing-the-environment",
    "title": "Tutorials",
    "section": "Initializing the environment",
    "text": "Initializing the environment\nSimilaritySearch.jl is writen in the Julia language you need to install it first in order to run them. After this it is necessary to install Pluto and/or IJulia (for Jupyter notebooks). If you need more information about how to install and use these notebooks, please see their respective sites.\n\nPluto\nIJulia"
  },
  {
    "objectID": "demos/syn2d.html",
    "href": "demos/syn2d.html",
    "title": "Visualizing MNIST database",
    "section": "",
    "text": "by: Eric S. Téllez\nThis demonstration shows in a 2D example the functionality of SearchGraph.\nusing SimilaritySearch, SimSearchManifoldLearning, Plots, StatsBase, LinearAlgebra, Markdown, Random\nn = 100_000\n\nM = randn(Float16, 2, n)\ndb = MatrixDatabase(M)\ndist = SqL2_asf32()\nsize(M)\nNow we can create the index\n1G = SearchGraph(; dist, db)\nctx = SearchGraphContext(hyperparameters_callback=OptimizeParameters(MinRecall(0.99)))\n2index!(G, ctx)\n3optimize_index!(G, ctx, MinRecall(0.9))\n\n\n1\n\nDefines the index and the search context (caches and hyperparameters); particularly, we use a very high quality build MinRecall(0.99); high quality constructions yield to faster queries due to the underlying graph structure.\n\n2\n\nActual indexing procedure using the given search context.\n\n3\n\nOptimizing the index to trade quality and speed."
  },
  {
    "objectID": "demos/syn2d.html#the-set-of-queries",
    "href": "demos/syn2d.html#the-set-of-queries",
    "title": "Visualizing MNIST database",
    "section": "The set of queries",
    "text": "The set of queries\nWe define a small set of queries being close to the border of the dataset and also in the most dense regions of the dataset.\n\nQ = [Float32[-2, -2], Float32[2, -2], Float32[-2, 0], Float32[-0, 2], Float32[0, 0],   Float32[-3, 3],  Float32[4, 4], Float32[1, 0.5]]\nI, D = searchbatch(G, ctx, VectorDatabase(Q), 30)\n\nPlease note how queries in low and high dense regions are located.\n\nscatter(view(M, 1, :), view(M, 2, :), fmt=:png, c=:cyan, ma=0.3, a=0.3, ms=1, msw=0)\n\nscatter!(getindex.(Q, 1), getindex.(Q, 2), c=:red, ma=0.7, a=0.7, ms=6, msw=0)\n\nfor c in eachcol(I)\n    X = M[:, c]\n    scatter!(view(X, 1, :), view(X, 2, :), c=:blue, ma=0.5, a=0.5, ms=2, msw=0)\n    #scatter!( c=:auto, ms=2)\nend\n\nplot!(legend=nothing)\n\nSince points are distributed in several regions with disparate density, their radii are also quite diverse. The next figure illustrates this effect."
  },
  {
    "objectID": "demos/syn2d.html#distribution-of-distances-for-the-set-of-queries",
    "href": "demos/syn2d.html#distribution-of-distances-for-the-set-of-queries",
    "title": "Visualizing MNIST database",
    "section": "Distribution of distances for the set of queries",
    "text": "Distribution of distances for the set of queries\n\nplot(D, m=:auto, yscale=:log10, title=\"knn distances for elements in Q\", fmt=:png)"
  },
  {
    "objectID": "demos/syn2d.html#environment-and-dependencies",
    "href": "demos/syn2d.html#environment-and-dependencies",
    "title": "Visualizing MNIST database",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [27ebfcd6] Primes v0.5.7\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "demos/primes-manifoldlearning.html",
    "href": "demos/primes-manifoldlearning.html",
    "title": "Prime numbers",
    "section": "",
    "text": "by: Eric S. Téllez\nThis demonstration is about prime numbers and its similarity based on its factors. This is a well-known demonstration of SimSearchManifoldLearning.jl. This notebook does not requires to download any dataset.\nNote: This example needs a lot of computing power; therefore you may want to set the environment variable JULIA_NUM_THREADS=auto before running julia.\nusing SimilaritySearch, SimSearchManifoldLearning, Primes, Plots, StatsBase, LinearAlgebra, Markdown, Random\nNow, we can define our dataset. The set of factors are found using the Primes package. Note that we use VectorDatabase to represent the dataset.\nn = 100_000\nF = Vector{Vector{Int32}}(undef, n+1)\n\nfunction encode_factors(num)\n    sort!(Int32[convert(Int32, f) for f in factor(Set, num)])\nend\n\nF[1] = Int32[1]\n\nfor i in 2:n+1\n    s = encode_factors(i)\n    F[i] = s\nend\n\ndb = VectorDatabase(F)\n#dist = JaccardDistance()  # Other distances from SimilaritySearch\n#dist = IntersectionDissimilarity()\n#dist = CosineDistanceSet()\ndist = DiceDistance()\nWe use Int32 ordered arrays to store prime factors to represent each integer. In the following cell define the cosine distance equivalent for this representation. While other representations may perform faster, this is quite straighforward and demonstrates the use of user’s defined distance functions."
  },
  {
    "objectID": "demos/primes-manifoldlearning.html#index-construction",
    "href": "demos/primes-manifoldlearning.html#index-construction",
    "title": "Prime numbers",
    "section": "Index construction",
    "text": "Index construction\nNote that the primes factors are pretty large for some large \\(n\\) and this imply challengues for metric indexes (since it is related with the intrinsic dimension of the dataset). We used a kernel that starts 64 threads, it solves \\(100000\\) in a few seconds but it can take pretty large time using single threads and larger \\(n\\) values. The construction of the index is used by the visualization algorithm (UMAP) to construct an all-knn graph, which can be a quite costly procedure.\n\nG = SearchGraph(; db, dist)\n1ctx = SearchGraphContext(hyperparameters_callback=OptimizeParameters(MinRecall(0.95)))\n2index!(G, ctx)\n3optimize_index!(G, ctx, MinRecall(0.9))\n\n\n1\n\nDefines the index and the search context (caches and hyperparameters); particularly, we use a very high quality build MinRecall(0.95); high quality constructions yield to faster queries due to the underlying graph structure.\n\n2\n\nActual indexing procedure using the given search context.\n\n3\n\nOptimizing the index to trade quality and speed."
  },
  {
    "objectID": "demos/primes-manifoldlearning.html#visualizing-with-umap-projection",
    "href": "demos/primes-manifoldlearning.html#visualizing-with-umap-projection",
    "title": "Prime numbers",
    "section": "Visualizing with UMAP projection",
    "text": "Visualizing with UMAP projection\nWe select to initialize the embedding randomly, this could yield to low quality embeddings, but it is much faster than other techniques like spectral layout. Note that we pass the Search graph G. We also use a second call to compute a 3D embedding for computing a kind of colour embedding, here we pass U2 to avoid recomputing several of the involved structures.\n\nfunction normcolors(V)\n    min_, max_ = extrema(V)\n    V .= (V .- min_) ./ (max_ - min_)\n    V .= clamp.(V, 0, 1)\nend\n\nnormcolors(@view e3[1, :])\nnormcolors(@view e3[2, :])\nnormcolors(@view e3[3, :])\n\nlet C = [RGB(c[1], c[2], c[3]) for c in eachcol(e3)],\n    X = view(e2, 1, :),\n    Y = view(e2, 2, :)\n    scatter(X, Y, color=C, fmt=:png, alpha=0.2, size=(600, 600), ma=0.3, ms=2, msw=0, label=\"\", yticks=nothing, xticks=nothing, xaxis=false, yaxis=false)\nend\n\nplot!()\n\n\n\n\n\ne2, e3 = let min_dist=0.5f0,\n             k=20,\n             n_epochs=75,\n             neg_sample_rate=3,\n             tol=1e-3,\n             layout=SpectralLayout()\n\n    @time \"Compute 2D UMAP model\" U2 = fit(UMAP, G; k, neg_sample_rate, layout, n_epochs, tol, min_dist)\n    @time \"Compute 3D UMAP model\" U3 = fit(U2, 3; neg_sample_rate, n_epochs, tol)\n    @time \"predicting 2D embeddings\" e2 = clamp.(predict(U2), -10f0, 10f0)\n    @time \"predicting 3D embeddings\" e3 = clamp.(predict(U3), -10f0, 10f0)\n    e2, e3\nend"
  },
  {
    "objectID": "demos/primes-manifoldlearning.html#environment-and-dependencies",
    "href": "demos/primes-manifoldlearning.html#environment-and-dependencies",
    "title": "Prime numbers",
    "section": "Environment and dependencies",
    "text": "Environment and dependencies\n\n\nJulia Version 1.10.9\nCommit 5595d20a287 (2025-03-10 12:51 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 64 × Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, cascadelake)\nThreads: 64 default, 0 interactive, 32 GC (on 64 virtual cores)\nEnvironment:\n  JULIA_PROJECT = .\n  JULIA_NUM_THREADS = auto\n  JULIA_LOAD_PATH = @:@stdlib\nStatus `~/sites/SimilaritySearchDemos/Project.toml`\n  [944b1d66] CodecZlib v0.7.8\n  [a93c6f00] DataFrames v1.7.0\n  [c5bfea45] Embeddings v0.4.6\n  [f67ccb44] HDF5 v0.17.2\n  [b20bd276] InvertedFiles v0.8.0 `~/.julia/dev/InvertedFiles`\n  [682c06a0] JSON v0.21.4\n  [eb30cadb] MLDatasets v0.7.18\n  [06eb3307] ManifoldLearning v0.9.0\n⌃ [ca7969ec] PlotlyLight v0.11.0\n  [91a5bcdd] Plots v1.40.11\n  [27ebfcd6] Primes v0.5.7\n  [ca7ab67e] SimSearchManifoldLearning v0.3.0 `~/.julia/dev/SimSearchManifoldLearning`\n  [053f045d] SimilaritySearch v0.12.0 `~/.julia/dev/SimilaritySearch`\n⌅ [2913bbd2] StatsBase v0.33.21\n  [7f6f6c8a] TextSearch v0.19.0 `~/.julia/dev/TextSearch`\nInfo Packages marked with ⌃ and ⌅ have new versions available. Those with ⌃ may be upgradable, but those with ⌅ are restricted by compatibility constraints from upgrading. To see why use `status --outdated`"
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Our demonstrations are Pluto and Jupyter notebooks that can be used to replicate and interactively use SimilaritySearch. To make the demonstrations more attractive, we also make intensive use of visualizations based on non-linear dimensional reduction, These kind of algorithms use k nearest neighbors of a database as input to produce the low dimensional embedding. In particular, we use the SimSearchManifoldLearning which provides an implementation of UMAP and also defines the necessary functions to interoperate with the ManifoldLearning package."
  },
  {
    "objectID": "tutorials/index.html#introduction",
    "href": "tutorials/index.html#introduction",
    "title": "Tutorials",
    "section": "",
    "text": "Our demonstrations are Pluto and Jupyter notebooks that can be used to replicate and interactively use SimilaritySearch. To make the demonstrations more attractive, we also make intensive use of visualizations based on non-linear dimensional reduction, These kind of algorithms use k nearest neighbors of a database as input to produce the low dimensional embedding. In particular, we use the SimSearchManifoldLearning which provides an implementation of UMAP and also defines the necessary functions to interoperate with the ManifoldLearning package."
  },
  {
    "objectID": "tutorials/index.html#basic-usage",
    "href": "tutorials/index.html#basic-usage",
    "title": "Tutorials",
    "section": "Basic usage",
    "text": "Basic usage\n\nBasic usage - Euclidean distance, Random 2D points.\nIncremental construction - Manhattan distance, Random 8D points.\nAutomatic hyperparameter optimization - Squared Euclidean distance Random 16D points.\nSolving single queries - Euclidean distance, Random points.\nParallel construction and search - Euclidean distance, Random 2D points."
  },
  {
    "objectID": "tutorials/index.html#all-knn",
    "href": "tutorials/index.html#all-knn",
    "title": "Tutorials",
    "section": "All KNN",
    "text": "All KNN\n\nAll-KNN - Euclidean distance, Random points.\n\nWe provide two kinds of examples: - Pluto reactive notebooks that can run locally or online. - Jupyter notebooks are less reactive but they are great to visualize directly on github without running."
  },
  {
    "objectID": "tutorials/index.html#list-of-examples",
    "href": "tutorials/index.html#list-of-examples",
    "title": "Tutorials",
    "section": "List of examples",
    "text": "List of examples\nWe separate the examples by the kind of data, since some of the datasets are quite large and will require a lot of computer power. We also list how to connect SimilaritySearch with other packages that require solving k nearest neighbor queries.\n\nIndexing and visualizing synthetic and easily generated data\n\nSynthetic 8D: A tutorial-like Jupyter notebook that shows how to create an index on synthetic data and search it. Synthetic 8-dimensional dataset under L2.\nSynthetic 2D: A tutorial-like Jupyter notebook working on 2D synthetic dataset, also shows how the index works on different density regions of the database. Synthetic 2-dimensional dataset under L2.\n\n\n\nIndexing and visualizing real high dimensional datasets\nAll Pluto notebooks can work on mybinder and run without install anything in your computer, however, some examples uses large datasets and some of them require high computational resources (as many threads as you have); they could run slow in cloud computing services.\n\nIntegers as prime factors: A tutorial-like Jupyter notebook that produces an UMAP visualization of integers represented by its prime factors. It uses UMAP 2D and 3D projections. Very high dimension, based on the number of factors under the \\(n\\) integers; different user defined distances.\n\nPrimes\nPrimes (using ManifoldLearning)\n\nPrime gaps: Visualization of sequences of prime gaps to visualize them for searching patterns in this infinity source of objects. It uses 2D and 3D projections.\n\nSearch and UMAP projection Prime Gaps demo. It generates the dataset.\nThe end of Primes (using ManifoldLearning) contains a prime-gap visualization with Isomap.\n\nWIT: This example shows how to navigate, query, and visualize a small subset of the WIT dataset using Clip embeddings (vision & language). ~300K 512-dimensional vectors using the cosine distance.\n\nJupyter-based WIT demo, SimilaritySearch v0.10.\nPluto-based WIT demo, SimilaritySearch v0.8.\n\nGlove: Navigate and visualize semantic representations (Glove word embeddings), also can solve analogies. The vocabulary consists of 400K tokens represented as 100-dimensional vectors under the cosine distance.\n\nJupyter-based GloVe demo, SimilaritySearch v0.10.\nPluto-based GloVe demo, SimilaritySerach v0.8.\n\nMNIST: Navigation and visualization of the MNIST dataset of hand drawing numbers. It uses images directly as objects (28x28 matrices).\n\nJupyter-based MNIST demo, SimilaritySearch v0.10.\nPluto-based MNIST demo, SimilaritySearch v0.8.\nPluto-based MNIST animated projections, SimilaritySearch v0.8.\n\nWiktionary: Pluto notebook to navigate and query the Wiktionary vocabulary using Levenshtein distance (~1M words)\n\nJupyter-based Wiktionary demo, SimilaritySearch v0.10.\nPluto-based Wiktionary demo, SimilaritySearch v0.8.\n\nTweets: Pluto notebook to visualize a collection of Twitter’s Spanish messages with emojis using bag of words representations. 50K items.\n\nSearch and UMAP projection Emojispace demo.\n\n\nTODO: Cites and references ### Interoperating with other packages - Working with ManifoldLearning. This Pluto notebook implements the necessary structs and functions to solve knn queries for ManifoldLearning algorithms. We used two datasets, the first corresponding to the scurve and the second is for Prime gaps as time series."
  },
  {
    "objectID": "tutorials/index.html#search-demos-and-umap-visualization",
    "href": "tutorials/index.html#search-demos-and-umap-visualization",
    "title": "Tutorials",
    "section": "Search demos and UMAP visualization",
    "text": "Search demos and UMAP visualization\nThe demos are Pluto and [Jupyter](https://jupyter.org/] notebooks. Inside the repo’s root run the following commands.\n\n$ JULIA_NUM_THREADS=auto julia --project=.\n...\n\njulia&gt; using Pluto\n...\njulia&gt; Pluto.run(notebook=\"WIT/wit-demo.jl\")\n...\nor\n\n$ JULIA_NUM_THREADS=auto jupyter-lab .\nPlease recall that the first time you load a package Julia compiles it. Pluto notebooks also save its own environments and therefore it can use different package versions that those listed in the repo environment, which will cause installing and compiling packages the first time the notebooks run. Hopefully, this strategy improves the reproducibility at the cost of increasing loading times. Jupyter notebooks also contain the necessary package-manager instructions to improve reproducibility.\nNote: Pluto interface also allows loading notebooks, so you don’t need to exit and re-run to explore examples.\n\nVisualization\nMost visualizations are made with UMAP models using the SimSearchManifoldLearning package. These can be expensive and it is always recommended to run notebooks with all available threads."
  },
  {
    "objectID": "tutorials/index.html#initializing-the-environment",
    "href": "tutorials/index.html#initializing-the-environment",
    "title": "Tutorials",
    "section": "Initializing the environment",
    "text": "Initializing the environment\nSimilaritySearch.jl is writen in the Julia language you need to install it first in order to run them. After this it is necessary to install Pluto and/or IJulia (for Jupyter notebooks). If you need more information about how to install and use these notebooks, please see their respective sites.\n\nPluto\nIJulia"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorials and Examples for the SimilaritySearch.jl package",
    "section": "",
    "text": "Here you will find several examples for the SimilaritySearch.jl package."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Tutorials and Examples for the SimilaritySearch.jl package",
    "section": "Installation",
    "text": "Installation\nYou need a Julia installation https://julialang.org/downloads/, in particular, we recommend to use Julia \\(v1.10\\) since \\(v1.11\\) has several performance regressions with respect to SimilaritySearch.\nWe present our examples just to copy and paste on the REPL but also provide some in Jupyter and Pluto notebooks; you must install IJulia and Pluto packages, just run the following commands in the REPL\nusing Pkg; Pkg.add([\"IJulia\", \"Pluto\"])\nPlease check their documentation:\n\nIJulia.\nJupyter.\nPluto\n\nThis website was made with Quarto with the Julia engine."
  },
  {
    "objectID": "index.html#notes-about-multithreading",
    "href": "index.html#notes-about-multithreading",
    "title": "Tutorials and Examples for the SimilaritySearch.jl package",
    "section": "Notes about multithreading",
    "text": "Notes about multithreading\nNearest neighbor search can be computationally expensive, therefore SimilaritySearch has multithreading support. You should want to run jupyter or julia using all available threads, that is\nJULIA_NUM_THREADS=auto jupyter-lab .\nor\nJULIA_NUM_THREADS=auto julia\nPerhaps all your threads may become your computer useless for a while, so you can replace auto by some other more conservative number that allow you to work on the same computer."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Tutorials and Examples for the SimilaritySearch.jl package",
    "section": "News",
    "text": "News\n\nmarch 20th, 2025: the site is behind the SimilaritySearch API; I will be working on updating examples and moving the site to Quarto.\njune 5th, 2023: adds news section, some installation requirements, Jupyter notebookes were updated to work with the v0.10 and with the current julia release v1.9. I also moved most plots to Makie."
  },
  {
    "objectID": "index.html#problem-statement",
    "href": "index.html#problem-statement",
    "title": "Tutorials and Examples for the SimilaritySearch.jl package",
    "section": "Problem statement",
    "text": "Problem statement\nGiven a finite dataset, \\(S \\subseteq U\\) where \\(n = |S|\\), and a metric distance function \\(d\\) working with any pair of elements in \\(U\\), the similarity search problem consists on retrieving similar items to a given query \\(q\\), for example, the \\(k\\) most similar items to \\(q\\) in \\(S\\) (\\(k\\) nearest neighbors).\nAt first glance, the problem is simple since it can be solved using an exhaustive evaluation of all possible results \\(d(u_1, q), \\cdots, d(u_n, q)\\) (that is, for all \\(u_i \\in S\\)) and select those \\(k\\) items \\(\\{u_i\\}\\) having the least distance to \\(q\\). This solution is impractical when \\(n\\) is large or when the number of expected queries is high. In these cases, it is necessary to create a data structure that preprocess the dataset and reduce the cost of solving queries, it is often called a metric index. When the dataset is pretty large or the metric space is quite complex, sometimes we can loose the ability of retrieving the exact solution to gain speed, clearly, the approximation quality becomes a major concern and these approximate methods require a lot of knowledge to trade speed retrieval process also kept high the solution’s quality. Additionally, the amount of memory used by the index and the construction time are also concerns whenever \\(n\\) is big.\nThe SearchGraph index in the SimilaritySearch.jl package is a competitive alternative for solving search queries that automatically tune search speed and quality and also remains very competitive in memory and construction costs. Here we show some demostrations of how using SimilaritySearch.jl in several synthetic and real problems."
  }
]